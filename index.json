[{"uri":"https://thanhtu18.github.io/workshop-template/4-eventparticipated/4.1-event1/","title":"Event 1","tags":[],"description":"","content":"Summary Report: ‚ÄúAI/ML/GenAI on AWS‚Äù Event Objectives Provide an overview of the AI/ML/GenAI ecosystem on AWS Guide participants through the full ML lifecycle using Amazon SageMaker Explain and demo Foundation Models on Amazon Bedrock Present Prompt Engineering, RAG, and Bedrock Agents techniques Build a GenAI chatbot through a live demo Agenda-based Highlights 8:30 ‚Äì 9:00 AM | Welcome \u0026amp; Introduction Main activities:\nParticipant check-in and networking Workshop learning objectives Ice-breaker activity Overview of the AI/ML landscape in Vietnam Key takeaway: Gained a clearer view of AI/ML trends in Vietnamese enterprises and how AWS supports digital transformation with GenAI and ML.\n9:00 ‚Äì 10:30 AM | AWS AI/ML Services Overview Main content:\nIntroduction to Amazon SageMaker as an end-to-end ML platform Data preparation and labeling Model training, tuning, and deployment Integrated MLOps in SageMaker Live Demo: SageMaker Studio walkthrough Key takeaway: Understood the full ML development lifecycle on AWS, how to prepare data, monitor training/tuning/deployment, grasped real-world MLOps, and experienced SageMaker Studio and workflow hands-on.\n10:30 ‚Äì 10:45 AM | Coffee Break Short networking and discussion break with AWS experts and other participants.\n10:45 AM ‚Äì 12:00 PM | Generative AI with Amazon Bedrock Main content:\nFoundation Models: Claude, Llama, Titan ‚Äî comparison \u0026amp; selection guide Prompt Engineering: Techniques, Chain-of-Thought reasoning, Few-shot learning RAG (Retrieval-Augmented Generation): Architecture, Knowledge Base integration Bedrock Agents: Multi-step workflows, tool integrations Guardrails: Safety, content filtering Live Demo: Building a GenAI chatbot with Amazon Bedrock Key takeaway: Learned how to select the right Foundation Model, master prompt engineering (CoT, few-shot), build a complete RAG system, use Bedrock Agents for multi-step workflows, understand content safety standards and Guardrails, and follow the full GenAI chatbot creation process.\nKey Takeaways Comprehensive understanding of ML and GenAI on AWS Understood the differences and use cases for FM models (Claude/Llama/Titan) Applied Chain-of-Thought and Few-shot to improve output quality Learned to design prompts for complex pipelines Understood why RAG is needed in enterprise apps and how to connect Bedrock to Knowledge Base Learned to build agents with tool integration Hands-on experience with SageMaker Studio and end-to-end ML workflow Understood how to deploy enterprise AI chatbots using AWS standards Applying to Work Apply RAG to internal chatbots or document support systems Use SageMaker to train/fine-tune ML models Use Prompt Engineering to improve FM output quality Integrate Bedrock Agents to automate workflows Build GenAI demos for team/project Event Experience Attending ‚ÄúAI/ML/GenAI on AWS‚Äù was an extremely valuable experience, deepening my understanding of how enterprises implement AI/ML and GenAI in practice.\nHighlights Learning from AWS experts: Clear analysis of the AI/ML roadmap for Vietnamese enterprises, real-world demos of SageMaker and Bedrock. Hands-on demo sessions: Directly observed the train ‚Üí tune ‚Üí deploy process, Bedrock chatbot demo clarified the GenAI app building workflow. Networking \u0026amp; Discussions: Opportunities to discuss with AWS engineers and other participants, learn from real GenAI case studies. Lessons learned: GenAI is not just a model but a complete workflow (Prompt ‚Üí RAG ‚Üí Agents ‚Üí Guardrails), SageMaker standardizes the ML lifecycle, and model selection is crucial for efficiency and cost. Some event photos Overall, the event not only provided technical knowledge but also helped me reshape my thinking about AI/ML application, system modernization, and more effective cross-team collaboration.\n"},{"uri":"https://thanhtu18.github.io/workshop-template/5-workshop/5.8-cognito/5.8.1-create-user-pool/","title":"Create Cognito User Pool","tags":[],"description":"","content":"Create User Pool In this step, you will create a Cognito User Pool to manage user authentication for your application.\nNavigate to AWS Cognito service in the AWS Console Click Create user pool Configure options\nOptions for sign-in identifiers:\nSelect Email (allows users to sign in with email) Uncheck Phone number and Username Self-registration:\n‚úÖ Enable self-registration (allows users to sign up themselves) Required attributes for sign-up:\nSelect email and name as required attributes Click Create user pool Enter User Pool Name\nUser pool name: TaskManagementUserPool Click Create user pool Verify User Pool Creation After successful creation, you should see your User Pool in the Cognito console with default configurations:\nReview Default Configuration Cognito automatically creates the User Pool with default settings:\nSign-in experience:\nSign-in options: Username (can be changed to Email) Password policy: Cognito defaults Multi-factor authentication: Optional Sign-up experience:\nSelf-service sign-up: Enabled Email verification: Enabled Required attributes: email Message delivery:\nEmail provider: Send email with Cognito App integration:\nHosted UI: Not configured (will be configured in later steps) Important Note ‚ö†Ô∏è Warning: Options for sign-in identifiers and required attributes cannot be changed after the User Pool is created. If you need to change these, you must create a new User Pool.\nNote down the User Pool ID from the overview page as you\u0026rsquo;ll need it for the next steps:\nIn the following steps, we will configure detailed features like password policies, email verification, and create App Clients.\n"},{"uri":"https://thanhtu18.github.io/workshop-template/5-workshop/5.1-workshop-overview/","title":"Introduction","tags":[],"description":"","content":"Building the TaskHub Platform with the DevSecOps Model on AWS Workshop Introduction In this workshop, you will build the entire TaskHub platform following the AWS Serverless + DevSecOps model, based on real-world architectures widely adopted by modern enterprises to achieve scalability, security, and cost efficiency.\nThe workshop is divided into groups of AWS services, helping you:\nUnderstand the role of each service in a modern serverless architecture. Deploy API Gateway, Lambda, DynamoDB, Cognito, and S3/CloudFront hands-on. Apply DevSecOps practices using CodePipeline, CodeBuild, and CodeGuru. Implement security at both the Backend (KMS, Secrets Manager) and Edge (WAF, Shield). Fully integrate a Next.js frontend with an AWS serverless backend. Architectural Overview In this workshop, you will build all the key components of the TaskHub platform:\nAmazon S3 ‚Äì Stores the static build of the Next.js application. Amazon CloudFront ‚Äì Delivers the UI globally with low latency. AWS WAF \u0026amp; AWS Shield ‚Äì Protect the application from DDoS attacks and OWASP Top 10 threats. Amazon Cognito ‚Äì Handles user authentication, identity management, and Admin/Member role authorization. Amazon API Gateway ‚Äì Serves as the entry point for frontend requests. AWS Lambda (Node.js/TypeScript) ‚Äì Processes all business logic. Amazon DynamoDB ‚Äì Stores tasks, users, and progress data. AWS KMS ‚Äì Encrypts data stored in DynamoDB. AWS Secrets Manager ‚Äì Stores sensitive information and API keys securely. AWS CodePipeline ‚Äì Automates the entire CI/CD process. AWS CodeBuild ‚Äì Builds frontend/backend and performs security scans. AWS CodeGuru Reviewer ‚Äì Analyzes code quality and provides optimization recommendations. AWS CloudFormation ‚Äì Deploys infrastructure via IaC. Amazon CloudWatch Logs ‚Äì Captures logs from Lambda and API Gateway. AWS X-Ray ‚Äì Provides system-wide tracing and latency analysis. Amazon SNS ‚Äì Sends system events and alert notifications. "},{"uri":"https://thanhtu18.github.io/workshop-template/1-worklog/1.1-week1/","title":"Week 1 Worklog","tags":[],"description":"","content":"Week 1 Objectives: Connect and get acquainted with members of First Cloud Journey. Understand basic AWS services, how to use the console \u0026amp; CLI. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 1 - Meeting AWS members and admin - Enjoy AWS event - Find member and create group project 06/09/2025 06/09/2025 2 - Module 01 + What is Cloud Computing? + What Makes AWS Different? + How to Start Your Journey to the Cloud + AWS Global Infrastructure + AWS Services Management Tools + Optimizing Costs on AWS and Working With\u0026hellip; + Additional Practice and Research 10/09/2025 10/09/2025 https://000001.awsstudygroup.com/ 3 - Create new AWS account - MFA for AWS Accounts - Create Admin Group and Admin User - Account Authentication Support - Explore and Configure AWS Management Console - Creating Support Cases and Case Management in AWS 11/09/2025 11/09/2025 https://000001.awsstudygroup.com/ 4 - Learn about AWS budget + Create budget by template + Create cost budget + Create usage budget + Create RI Budget + Create Savings Plans Budget + Clean Up Budgets 12/09/2025 12/09/2025 https://000007.awsstudygroup.com/ 5 - Learn about AWS Support Packages - Access AWS Support + Types of Support Requests + Change support package - Manage Support Requests + Create Support Request + Select severity 13/09/2025 13/09/2025 https://000009.awsstudygroup.com/ 6 - Module 02 + AWS Virtual Private Cloud + VPC Security and Multi-VPC features + VPN - DirectConnect - Load Balancer\u0026hellip; - Start with Amazon VPC and AWS VPN Site-to-Site - Firewall in VPC + Security Group + Network ACLs + VPC Resource Map - Preparation Steps + Create VPC + Create Subnet + Create Internet Gateway + Create Route Table + Create Security Group + Enable VPC Flow Logs 14/09/2025 14/09/2025 https://www.youtube.com/watch?v=O9Ac_vGHquM\u0026list=PLahN4TLWtox2a3vElknwzU_urND8hLn1i\u0026index=25 https://www.youtube.com/watch?v=BPuD1l2hEQ4\u0026list=PLahN4TLWtox2a3vElknwzU_urND8hLn1i\u0026index=26 https://www.youtube.com/watch?v=CXU8D3kyxIc\u0026list=PLahN4TLWtox2a3vElknwzU_urND8hLn1i\u0026index=27 https://000003.awsstudygroup.com/ üèÜ Week 1 Achievements 1. Networking \u0026amp; Collaboration\nMet AWS members and administrators Joined an AWS event Formed a group project team 2. Module 01 ‚Äì AWS Cloud Fundamentals\nLearned about Cloud Computing and AWS Global Infrastructure Explored Management Tools and Cost Optimization strategies Created and configured a new AWS account with MFA, Admin Group/User Practiced Support Case Management via the AWS Console 3. AWS Budgets \u0026amp; Support\nCreated and managed AWS Budgets (Cost, Usage, RI, Savings Plans) Explored AWS Support Packages and learned Support Request workflows Practiced setting severity levels and upgrading support plans 4. Module 02 ‚Äì AWS VPC\nCreated VPC, Subnets, Route Tables, and Security Groups Enabled VPC Flow Logs for traffic monitoring Learned about VPN, Direct Connect, Load Balancer, and VPC Security (SG, NACL, Firewall) "},{"uri":"https://thanhtu18.github.io/workshop-template/","title":"Internship Report","tags":[],"description":"","content":"Internship Report Student Information: Full Name: Dang Thanh Tu\nPhone Number: 0799747272\nEmail: tudtse184093@fpt.edu.vn\nUniversity: FPT University\nMajor: Information Technology\nClass: AWS092025\nInternship Company: Amazon Web Services Vietnam Co., Ltd.\nInternship Position: FCJ Cloud Intern\nInternship Duration: From 08/09/2025 to 24/12/2025\nReport Content Worklog Proposal Translated Blogs Events Participated Workshop Self-evaluation Sharing and Feedback "},{"uri":"https://thanhtu18.github.io/workshop-template/1-worklog/","title":"Worklog","tags":[],"description":"","content":"On this page, I will give an overview of my work log: I completed the internship program within 12 weeks, each week I studied theory, did labs and recorded what I did with other AWS services.\nThe work log was done over about 3 months, so I divided the content into 12 weeks as below so that I can know what I did in each week and what AWS knowledge and skills I learned.\nWeek 1: Understand basic AWS services, how to use the console \u0026amp; CLI.\nWeek 2: Set up Hybrid DNS with Route 53 Resolver and VPC peering\nWeek 3: Set up AWS Transit Gateway. Create Transit Gateway Attachments and Route Tables. Learn Amazon EC2 comprehensive concepts and services. Study EC2 Auto Scaling, EFS/FSx, Lightsail, and MGN\nWeek 4: Deploy AWS Backup to automate data protection. Learn AWS Storage Gateway for hybrid cloud storage. Start with Amazon S3 fundamentals and static website hosting\nWeek 5: DMaster AWS Storage Services and Amazon S3 comprehensive features. Learn AWS Backup and VM Import/Export strategies. Practice Storage Gateway for hybrid cloud storage solutions\nWeek 6: DUnderstand Amazon FSx for Windows File Server architecture and use cases. Deploy and configure multi-AZ FSx file systems (SSD \u0026amp; HDD). Practice performance testing, monitoring, and optimization on FSx\nWeek 7: Understand Amazon S3 fundamentals and main use cases. Create and configure S3 buckets for static website hosting. Practice access control, CloudFront integration, and versioning Week 8: DReview AWS shared responsibility model and core security/IAM services. Learn and practice AWS Security Hub with security standards. Learn how to optimize EC2 costs using AWS Lambda automation Week 9: Learn how to use tags to manage AWS resources. Practice creating Resource Groups and filtering resources by tags. Manage access to EC2 using resource tags and IAM policies. Understand IAM permission boundaries and how to limit user permissions\nWeek 10: Practice encryption at rest using AWS KMS with S3 and CloudTrail/Athena. Review IAM roles, conditions, and access control patterns. Practice IAM roles for applications (EC2 -\u0026gt; S3). Learn core AWS database services: Amazon RDS, Aurora, Redshift, ElastiCache\nWeek 11: Practice building and managing relational databases using Amazon RDS. Explore database migration tools and scenarios with Lab 43. Build a simple data lake on AWS with S3, Glue, Athena, and QuickSight. Learn to create and work with Amazon DynamoDB, including backups and migration\nWeek 12: Practice building and inspecting cost-related data in AWS. Get familiar with different ways to interact with AWS (CloudShell, Console, SDK). Use AWS Glue DataBrew and Cloud9 to prepare and transform data. Build an end-to-end analytics pipeline with Glue, EMR, Athena, Kinesis Data Analytics, QuickSight, Lambda, and Redshift\n"},{"uri":"https://thanhtu18.github.io/workshop-template/3-blogstranslated/3.1-blog1/","title":"Blog 1","tags":[],"description":"","content":"Develop and monitor a Spark application using existing data in Amazon S3 with Amazon SageMaker Unified Studio by Amit Maindola and Abhilash Nagilla | on July 9, 2025 | in Amazon SageMaker Data \u0026amp; AI Governance, Amazon SageMaker Lakehouse, Amazon SageMaker Unified Studio, Analytics, Technical How-to\nOrganizations face significant challenges managing their big data analytics workloads. Data teams struggle with fragmented development environments, complex resource management, inconsistent monitoring, and cumbersome manual scheduling processes. These issues lead to lengthy development cycles, inefficient resource utilization, reactive troubleshooting, and difficult-to-maintain data pipelines. These challenges are especially critical for enterprises processing terabytes of data daily for business intelligence (BI), reporting, and machine learning (ML). Such organizations need unified solutions that streamline their entire analytics workflow.\nThe next generation of Amazon SageMaker combined with Amazon EMR in Amazon SageMaker Unified Studio addresses these pain points through an integrated development environment (IDE) where data workers can develop, test, and refine Spark applications in a consistent environment. Amazon EMR Serverless reduces the cluster management burden by dynamically provisioning resources based on workload requirements, and integrated monitoring tools help teams quickly identify performance bottlenecks.\nIntegration with Apache Airflow through Amazon Managed Workflows for Apache Airflow (Amazon MWAA) provides robust scheduling capabilities, and the pay-only-for-resources-used model delivers significant cost savings.\nIn this post, we demonstrate how to develop and monitor a Spark application using existing data in Amazon Simple Storage Service (Amazon S3) with SageMaker Unified Studio.\nSolution overview This solution uses SageMaker Unified Studio to execute and monitor Spark applications, highlighting integration capabilities. We demonstrate the following key steps:\nCreate an EMR Serverless compute environment for interactive applications using SageMaker Unified Studio Create and configure Spark applications Use TPC-DS data to build and run Spark applications using Jupyter notebooks in SageMaker Unified Studio Monitor application performance and schedule periodic runs with Amazon MWAA integration Analyze results in SageMaker Unified Studio to optimize workflows Prerequisites To follow this tutorial, you need the following requirements:\nAn AWS account ‚Äì If you don\u0026rsquo;t have an account yet, you can create one A SageMaker Unified Studio domain ‚Äì For instructions, see Create an Amazon SageMaker Unified Studio domain ‚Äì quick setup A demo project ‚Äì Create a demo project in your SageMaker Unified Studio domain. For instructions, see Create a project. For this example, we choose a profile with All capabilities in the project configuration section Adding EMR Serverless to compute Complete the following steps to create an EMR Serverless compute environment for building Spark applications:\nIn SageMaker Unified Studio, open the project you created as a prerequisite and choose Compute Choose Data processing, then choose Add compute Choose Create new compute resources, then choose Next Choose EMR Serverless, then choose Next Enter a name for Compute name For Release label, choose emr-7.5.0 For Permission mode, choose Compatibility Choose Add compute The EMR Serverless application initialization process takes a few minutes. After creation is complete, you can view the compute in SageMaker Unified Studio. The above steps illustrate how you set up an Amazon EMR Serverless application in SageMaker Unified Studio to run interactive PySpark workloads. In the following steps, we will build and monitor the Spark application in the interactive JupyterLab workspace.\nDeveloping, monitoring, and debugging Spark applications in Jupyter notebooks In this section, we build a Spark application using the TPC-DS dataset in SageMaker Unified Studio. With Amazon SageMaker Data Processing, you can focus on transforming and analyzing data without having to manage compute capacity or open-source applications, saving time and reducing costs.\nSageMaker Data Processing provides a unified development experience from Amazon EMR, AWS Glue, Amazon Redshift, Amazon Athena, and Amazon MWAA in the same notebook and query interface. You can automatically provision capacity on Amazon Elastic Compute Cloud (Amazon EC2) or EMR Serverless. Autoscaling rules manage changing compute demands to optimize performance and execution time.\nImplementation steps: After completing the previous preparation steps, go to SageMaker Studio and open your project Choose Build, then choose JupyterLab The notebook takes about 30 seconds to initialize and connect to the workspace Under the Notebook category, choose Python 3 (ipykernel) In the first cell, next to Local Python, select the dropdown menu and choose PySpark Choose the dropdown menu next to Project.Spark and select the EMR-S Compute compute Run the following code to develop your Spark application. This example reads a 3 TB TPC-DS dataset in Parquet format from a public S3 bucket: spark.read.parquet(\u0026#34;s3://blogpost-sparkoneks-us-east-1/blog/BLOG_TPCDS-TEST-3T-partitioned/store/\u0026#34;).createOrReplaceTempView(\u0026#34;store\u0026#34;) When the Spark session starts and execution logs begin to appear, you can explore the Spark UI and driver logs to debug and troubleshoot your Spark program. The following screenshot shows an example of the Spark user interface. The following screenshot shows an example of driver logs. The following screenshot shows the Executors tab, providing access to driver and executor logs. Use the following code to read additional TPC-DS datasets. You can create temporary views and use the Spark UI to view the files being read. Refer to the appendix at the end of this article for details on how to use TPC-DS datasets in your buckets. spark.read.parquet(\u0026#34;s3://blogpost-sparkoneks-us-east-1/blog/BLOG_TPCDS-TEST-3T-partitioned/item/\u0026#34;).createOrReplaceTempView(\u0026#34;item\u0026#34;) spark.read.parquet(\u0026#34;s3://blogpost-sparkoneks-us-east-1/blog/BLOG_TPCDS-TEST-3T-partitioned/store_sales/\u0026#34;).createOrReplaceTempView(\u0026#34;store_sales\u0026#34;) spark.read.parquet(\u0026#34;s3://blogpost-sparkoneks-us-east-1/blog/BLOG_TPCDS-TEST-3T-partitioned/date_dim/\u0026#34;).createOrReplaceTempView(\u0026#34;date_dim\u0026#34;) spark.read.parquet(\u0026#34;s3://blogpost-sparkoneks-us-east-1/blog/BLOG_TPCDS-TEST-3T-partitioned/customer/\u0026#34;).createOrReplaceTempView(\u0026#34;customer\u0026#34;) spark.read.parquet(\u0026#34;s3://blogpost-sparkoneks-us-east-1/blog/BLOG_TPCDS-TEST-3T-partitioned/catalog_sales/\u0026#34;).createOrReplaceTempView(\u0026#34;catalog_sales\u0026#34;) spark.read.parquet(\u0026#34;s3://blogpost-sparkoneks-us-east-1/blog/BLOG_TPCDS-TEST-3T-partitioned/web_sales/\u0026#34;).createOrReplaceTempView(\u0026#34;web_sales\u0026#34;) In each notebook cell, you can open Spark Job Progress to view the stages of jobs sent to EMR Serverless for a specific cell. You can see the completion time for each stage. If errors occur, you can check the logs, making troubleshooting more seamless. Since the files are partitioned based on date key columns, you can observe that Spark runs tasks in parallel to read the data. 8. Next, get counts by date keys on data partitioned by time key using the following code:\nselect count(1), ss_sold_date_sk from store_sales group by ss_sold_date_sk order by ss_sold_date_sk Monitoring jobs in Spark UI In the Jobs tab of the Spark UI, you can view a list of completed or running jobs, with the following information:\nAction that triggered the job Execution time (e.g., 41 seconds, but times will vary) Number of stages and tasks ‚Äî in this example, 2 stages and 3,428 tasks You can select a job to see more details, especially about the stages. Our job has two stages; a new stage is created each time there is a shuffle. We have one stage to read data from each initial dataset, and one stage for aggregation. In the next example, we run some TPC-DS SQL queries used for performance evaluation and benchmarking:\nwith frequent_ss_items as (select substr(i_item_desc,1,30) itemdesc,i_item_sk item_sk,d_date solddate,count(*) cnt from store_sales, date_dim, item where ss_sold_date_sk = d_date_sk and ss_item_sk = i_item_sk and d_year in (2000, 2000+1, 2000+2,2000+3) group by substr(i_item_desc,1,30),i_item_sk,d_date having count(*) \u0026gt;4), max_store_sales as (select max(csales) tpcds_cmax from (select c_customer_sk,sum(ss_quantity*ss_sales_price) csales from store_sales, customer, date_dim where ss_customer_sk = c_customer_sk and ss_sold_date_sk = d_date_sk and d_year in (2000, 2000+1, 2000+2,2000+3) group by c_customer_sk) x), best_ss_customer as (select c_customer_sk,sum(ss_quantity*ss_sales_price) ssales from store_sales, customer where ss_customer_sk = c_customer_sk group by c_customer_sk having sum(ss_quantity*ss_sales_price) \u0026gt; (95/100.0) * (select * from max_store_sales)) select sum(sales) from (select cs_quantity*cs_list_price sales from catalog_sales, date_dim where d_year = 2000 and d_moy = 2 and cs_sold_date_sk = d_date_sk and cs_item_sk in (select item_sk from frequent_ss_items) and cs_bill_customer_sk in (select c_customer_sk from best_ss_customer) union all (select ws_quantity*ws_list_price sales from web_sales, date_dim where d_year = 2000 and d_moy = 2 and ws_sold_date_sk = d_date_sk and ws_item_sk in (select item_sk from frequent_ss_items) and ws_bill_customer_sk in (select c_customer_sk from best_ss_customer))) x You can monitor Spark jobs in SageMaker Unified Studio in two ways. Jupyter notebooks provide basic monitoring, showing real-time job status and execution progress. For more detailed analysis, use the Spark UI. You can check specific stages, tasks, and execution plans. The Spark UI is particularly useful for troubleshooting performance issues and optimizing queries. You can track the expected number of stages, running tasks, and detailed duration of each task. This comprehensive view helps you understand resource usage and track job progress at a detailed level.\nIn this section, we explained how you can use EMR Serverless compute in SageMaker Unified Studio to build interactive Spark applications. Through the Spark UI, interactive applications provide detailed task-level status, I/O and shuffle information, as well as links to corresponding task logs directly from the notebook, enabling a seamless debugging experience.\nCleanup To avoid ongoing charges in your AWS account, delete the resources you created in this tutorial:\nDelete connections Delete EMR jobs Delete EMR output S3 buckets Delete Amazon MWAA resources such as workflows and environments Conclusion In this post, we demonstrated how the next generation of SageMaker, combined with EMR Serverless, provides a powerful solution for developing, monitoring, and scheduling Spark applications using data in Amazon S3. The integrated experience significantly reduces complexity by providing a unified development environment, automatic resource management, and comprehensive monitoring capabilities through the Spark UI, while maintaining cost efficiency with the pay-as-you-use model. For enterprises, this means faster time to insights, improved team collaboration, and reduced operational overhead, allowing data teams to focus on analytics rather than infrastructure management.\nTo get started, explore the Amazon SageMaker Unified Studio User Guide, set up a project in your AWS environment, and discover how this solution can transform your organization\u0026rsquo;s data analytics capabilities.\nAppendix In the following sections, we discuss how to run scheduled workloads and provide details about the TPC-DS dataset for building Spark applications using EMR Serverless.\nRunning scheduled workloads In this section, we deploy JupyterLab notebooks and create workflows using Amazon MWAA. You can use workflows to orchestrate notebooks, querybooks, and many other things in your project repository. With workflows, you can define a set of tasks organized as a directed acyclic graph (DAG) that can run on a schedule you define. Implementation steps:\nIn SageMaker Unified Studio, choose Build, and under Orchestration, choose Workflows Choose Create Workflow in Editor You will be directed to the JupyterLab notebook with a new DAG named untitled.py created in the /src/workflows/dag folder\nRename this notebook to tpcds_data_queries.py\nYou can reuse the existing template with the following updates:\na. Update line 17 with the schedule you want your code to run on b. Update line 26 with your NOTEBOOK_PATH. This path should be in src/\u0026lt;notebook_name\u0026gt;.ipynb. Note that the dag_id name is auto-generated; you can name it as per your requirement. 5. Choose File and Save notebook To test, you can trigger a manual workload run 6. In SageMaker Unified Studio, choose Build, then under Orchestration, choose Workflows. 7. Choose your workflow, then choose Run. You can monitor job success on the Runs tab. To debug notebook jobs by accessing the Spark UI in the Airflow job console, you must use EMR Serverless Airflow Operators to submit jobs. The link is available on the Details tab of the query. This option has key limitations: it is not available for Amazon EMR on EC2, and SageMaker notebook job operators do not work.\nYou can configure the operator to create one-time links to the application UI and Spark output logs by passing enable_application_ui_links=True as a parameter. After the job starts running, these links are available on the Details tab of the corresponding task. If enable_application_ui_links=False, the links will appear but in a grayed-out state.\nMake sure you have emr-serverless:GetDashboardForJobRun in AWS Identity and Access Management (IAM) to create dashboard links.\nOpen the Airflow user interface for your task. The Spark user interface and history server dashboard options will appear on the Details tab, as shown in the following screenshot.\nThe screenshot illustrates the Jobs tab of the Spark UI.\nAbout the authors Amit Maindola is a Senior Data Architect focused on data engineering, analytics, and AI/ML at Amazon Web Services. He helps customers in their digital transformation journey and enables them to build highly scalable, robust, and secure cloud-based analytical solutions on AWS to gain timely insights and make critical business decisions.\nAbhilash Nagilla is a senior specialist solutions architect at Amazon Web Services (AWS), supporting public sector customers on their cloud journey with a focus on AWS data and AI services. Outside of work, Abhilash enjoys learning new technologies, watching movies, and traveling to new places.\n"},{"uri":"https://thanhtu18.github.io/workshop-template/2-proposal/","title":"Proposal","tags":[],"description":"","content":"TaskHub - Task and Progress Management Platform following the DevSecOps Model on AWS Proposal Document: View on Google Docs\n1. Executive Summary TaskHub is a task and progress management platform designed to help working groups or small to medium-sized businesses manage work, deadlines, and progress in a visual and secure manner.\nThe system is developed following the DevSecOps model, built entirely on AWS Serverless, ensuring scalability, security, and cost optimization.\nThe development and deployment process uses AWS CodePipeline and CodeBuild to automate CI/CD and security testing.\n2. Problem Statement Problem:\nSmall businesses and project teams often struggle with managing workload, tracking progress, and distributing tasks among members. Popular management tools like Jira or Asana often have high costs and lack seamless support for DevSecOps processes or the AWS environment. Solution:\nTaskHub uses a Serverless architecture on AWS to build a lightweight, secure, and cost-effective platform. The platform is developed using AWS Lambda, API Gateway, DynamoDB, Cognito, and S3/CloudFront, while integrating AWS CodePipeline for CI/CD and dynamic security testing. Benefits and Return on Investment (ROI)\nThe TaskHub solution brings many practical benefits for development teams and small to medium-sized businesses. The system serves as a central platform for managing tasks, tracking progress, and delegating authority to members effectively. The application of the serverless model on AWS helps minimize operating costs, optimize resources, and increase scalability as usage demand grows. Furthermore, this platform supports building a practical DevSecOps environment, paving the way for research and development teams to expand further projects. According to estimates from the AWS Pricing Calculator, the system\u0026rsquo;s operating cost is only about $0.66 per month, equivalent to $7.92 per year, while the entire initial infrastructure leverages shared services from AWS, with no additional hardware costs. The expected payback period is achieved within 6-12 months due to significantly reduced manual management work and optimized internal workflow.\n3. Solution Architecture The TaskHub platform is built based on AWS Serverless architecture, ensuring operational scalability, high performance, and cost-effective operation. The system focuses on task management, teamwork, and real-time project progress, while maintaining an automated development and deployment process through the DevSecOps model.\nThe overall architecture includes key components such as Amazon API Gateway taking responsibility for receiving and distributing user requests, AWS Lambda handling business logic backend and interacting with the Amazon DynamoDB database to store task information, users, and access permissions.\nThe web interface is hosted via Amazon S3 and distributed globally by Amazon CloudFront, while AWS Cognito ensures authentication and user authorization.\nThe CI/CD process is automated using AWS CodePipeline combined with AWS CodeBuild, enabling continuous development deployment and security testing without server management.\nThe entire architecture is protected by AWS WAF and AWS KMS to enhance data security and ensure additional DevSecOps compliance. AWS X-Ray is used to monitor performance and analyze latency. The overall architecture is described in detail in the diagram below:\nAWS Services Used Amazon Route 53: Highly reliable DNS service, routing traffic. AWS WAF (Web Application Firewall): Advanced protection layer, blocking common attacks. Amazon CloudFront: Global distribution of user interface and static content. Amazon S3 (Simple Storage Service): Static hosting of the entire web interface source code (Next.js build files). Amazon Cognito: User authentication and authorization management. Amazon API Gateway: Middleware communication layer, performing authentication and routing API requests to Lambda. AWS Lambda: Core business logic processing. Integrated to log activities into CloudWatch Logs. Amazon DynamoDB: High-performance NoSQL database. Data is encrypted using AWS KMS. AWS SNS (Simple Notification Service): Handles asynchronous notifications. AWS Secrets Manager: Secure storage, management, and rotation of secrets. AWS CodePipeline, CodeBuild, \u0026amp; CodeGuru: CodePipeline/CodeBuild: Building and automating the CI/CD process. CodeBuild runs automated tests and Static Application Security Testing (SAST). AWS CodeGuru: Automated source code analysis tool, integrated into the CI/CD process to provide intelligent recommendations for performance optimization and code quality improvement, especially important in the Lambda environment. AWS CloudFormation: Infrastructure as Code (IaC) service for deploying all resources. AWS CloudWatch Logs \u0026amp; AWS X-Ray: CloudWatch Logs collects logs. CloudWatch uses this data to set up alerts. AWS X-Ray provides in-depth request tracing capabilities. Component Design User Interface Layer (Frontend):\nInterface: Next.js application built as static files. Hosting \u0026amp; Distribution: Static files are securely stored in Amazon S3 (configured as Origin for CloudFront). This interface is distributed globally by Amazon CloudFront with low latency, while being protected by AWS WAF (Web Application Firewall) at the Edge layer. Business Logic Layer (Backend):\nAPI Gateway: Amazon API Gateway receives all requests. It is configured with Cognito Authorizer to validate user tokens before forwarding requests. Processing: Lambda Functions are responsible for handling business logic (CRUD tasks, team management, permissions). Secret Management: Each Lambda function accesses sensitive information (such as external API keys) through AWS Secrets Manager, ensuring secrets are never hard-coded. Data Layer (Database):\nDatabase: Amazon DynamoDB is used to store task data, progress, and user configuration. DynamoDB operates in On-Demand mode for automatic scaling and cost optimization. Data Security: All data at rest in DynamoDB is encrypted using keys managed by AWS KMS (Key Management Service), meeting the highest security standards. Security and Authentication:\nAuthentication: Amazon Cognito provides login mechanisms, session management, and Role-Based Access Control (RBAC) for users. Cognito also supports Multi-Factor Authentication (MFA) and SSO (Single Sign-On) integration. Edge Protection: AWS WAF is placed in front of CloudFront to prevent Layer 7 DDoS attacks and other common security vulnerabilities (OWASP Top 10). Deployment and Monitoring:\nCI/CD DevSecOps: Source code is stored on GitLab (as per the diagram) and automated via the AWS CodePipeline/CodeBuild chain. This process includes running CodeGuru to optimize code before deploying infrastructure via CloudFormation. Monitoring \u0026amp; Debugging: AWS CloudWatch Logs collects detailed logs from all services. AWS CloudWatch uses these logs to set up automatic alerts for errors or incidents. AWS X-Ray provides an overview of transaction flow performance, aiding in debugging and latency optimization. 4. Technical Implementation The development of the TaskHub project is divided into two main parts‚Äîbuilding the AWS serverless infrastructure and developing the task management platform‚Äîeach including the following key implementation phases:\nDeclared Development Phases\nPhase 1: Design and Modeling (Month 1)\nMain Action: Research Serverless/DevSecOps, select core services (Lambda, DynamoDB, API Gateway). Design detailed architecture diagrams and NoSQL data models. Deliverables: Architecture Diagram and Data Model Documentation. Phase 2: Infrastructure as Code Initialization (Month 2)\nMain Action: Calculate detailed operating costs. Use AWS CDK to build IaC source code for platform services (S3, CloudFront, Cognito), ensuring environment reproducibility. Deliverables: Base AWS CDK source code and Operating Cost Report. Phase 3: DevSecOps Automation Setup (Month 2-3)\nMain Action: Set up a complete CI/CD Pipeline (CodePipeline/CodeBuild). Integrate AWS CodeGuru and SAST tools to automate source code quality and security checks before deployment. Deliverables: Operational CodePipeline and automated security scanning process. Phase 4: Development and Deployment (Month 3-4)\nMain Action: Develop functionality (Lambda Functions with TypeScript) and interface (Next.js). Perform Integration Testing between services. Deploy the official production release via Pipeline. Deliverables: TaskHub Beta Version (complete CRUD) and Testing Report. Technical Requirements\nArchitecture and Tools: The entire system is declared and managed using AWS CDK to ensure infrastructure consistency. Technology: Backend uses TypeScript/Node.js. Frontend uses Next.js (React). Source Code Management: Source code on GitLab, automated deployment via AWS CodePipeline. Monitoring: Configure CloudWatch, X-Ray, and CloudWatch Logs for performance monitoring and in-depth debugging. Non-functional Requirements: The system is located in Singapore (ap-southeast-1) to optimize speed in Vietnam, capable of scaling up to 50 users, and uses AWS KMS for data encryption. 5. Timeline \u0026amp; Key Milestones Project Timeline Pre-internship (Month 0): Prepare plans, research DevSecOps and AWS Serverless services. Month 1: Set up development environment, initiate AWS infrastructure and CI/CD pipeline. Month 2: Design architecture, develop core functionality, and automate security testing. Month 3: Integrate frontend-backend, develop testing, and launch the platform. Post-launch: Maintenance, performance evaluation, and expansion of advanced features. 6. Budget Estimate Resource Responsibility Rate (USD) / Hour Solution Architects [1 person] System Architecture Design, API design, Database Schema, Technical Leadership 6 Engineers [3 person] Backend Development, Frontend Development, Security Implement 4 Other (DevOps) [1 person] CI/CD, Cloud Deployment, Monitoring, Security, Security Configuration 4 Project Phase Solution Architects Engineers Other (Please specify) Total Hours System Design \u0026amp; Architecture 20 10 0 30 Backend Development 10 80 0 90 Frontend Development 5 60 0 65 Security \u0026amp; CI/CD Setup 5 30 10 45 Testing \u0026amp; Deployment 5 30 0 35 Total Hours 45 210 10 265 Total Cost (USD) 270 840 40 800 Cost Contribution distribution between Partner, Customer, AWS:\nParty Contribution (USD) % Contribution of Total Customer 0 0 Partner 0 0 AWS 800 100 7. Risk Assessment Risk Matrix\nNetwork or AWS service disruption: Medium impact, medium probability. CI/CD deployment errors: High impact, low probability. Exceeding AWS budget: Medium impact, low probability. Security vulnerabilities: High impact, medium probability. Performance degradation under load: Medium impact, medium probability. Mitigation Strategies\nUse AWS multi-region and monitor with CloudWatch/X-Ray. Test and review source code before deployment via CodePipeline. Set up cost alerts via AWS Budgets. Perform automated security scanning using CodeBuild (replacing GitHub Actions). Contingency Plan\nMaintain a staging environment for quick recovery. Use CloudFormation and AWS Backup for configuration and data backup. 8. Expected Outcomes Technical Improvements\nComprehensive Automation: Complete transition to automated DevSecOps process. New system deployment time takes under 6 minutes. Performance Guarantee: Fast application operation (API response under 150ms) and stability (99.9% Uptime) thanks to Serverless architecture. Integrated Security: Automated scanning and remediation of high-level security vulnerabilities during the Code Build process. Scalability Ready: The platform can scale to serve many users and handle large traffic volumes without structural changes. Long-term Value\nTechnical Asset Creation: Creation of a complete AWS CDK/CloudFormation codebase. This is a cost-optimized Serverless architecture template that can be reused for other projects by the team. Robust Platform Foundation: Establishment of a work management and development environment following industrial standards (DevSecOps), ready for future feature expansion. "},{"uri":"https://thanhtu18.github.io/workshop-template/4-eventparticipated/4.2-event2/","title":"Event 2","tags":[],"description":"","content":"Summary Report: \u0026ldquo;AWS Cloud Mastery Series #2 - DevOps on AWS\u0026rdquo; Event Objectives Learn DevOps culture and principles in cloud environment Build complete CI/CD pipeline with AWS DevOps Services Master Infrastructure as Code (IaC) with CloudFormation and CDK Understand Container Services on AWS (ECR, ECS, EKS, App Runner) Implement monitoring and observability with CloudWatch and X-Ray Apply DevOps best practices and learn from real case studies Agenda-based Highlights 8:30 ‚Äì 9:00 | Welcome \u0026amp; DevOps Mindset Main activities:\nCheck-in and recap of previous AI/ML session Introduction to DevOps culture and principles Benefits and key metrics (DORA, MTTR, deployment frequency) Importance of DevOps in digital transformation Key takeaway: Understood that DevOps is not just tools but a working culture, grasped key metrics to measure DevOps effectiveness and how to apply them in practice.\n9:00 ‚Äì 10:30 | AWS DevOps Services ‚Äì CI/CD Pipeline Main content:\nSource Control: AWS CodeCommit and Git strategies (GitFlow, Trunk-based) Build \u0026amp; Test: CodeBuild configuration, automated testing pipelines Deployment: CodeDeploy with Blue/Green, Canary, and Rolling updates Orchestration: Automation with CodePipeline Live Demo: Complete CI/CD pipeline walkthrough Key takeaway: Mastered the process of building CI/CD pipeline from source code to deployment, understood different deployment strategies, and gained hands-on experience with AWS DevOps tools.\n10:30 ‚Äì 10:45 | Break Networking and discussion with AWS experts and other participants.\n10:45 ‚Äì 12:00 | Infrastructure as Code (IaC) Main content:\nAWS CloudFormation: Templates, stacks, and drift detection AWS CDK (Cloud Development Kit): Constructs, reusable patterns, and language support Demo: Deployment with CloudFormation and CDK Discussion: Choosing appropriate IaC tools Key takeaway: Deep understanding of Infrastructure as Code, comparison between CloudFormation and CDK, and learned how to apply IaC to manage infrastructure efficiently and reusably.\n12:00 ‚Äì 13:00 | Lunch Break (Self-arranged) 13:00 ‚Äì 14:30 | Container Services on AWS Main content:\nDocker Fundamentals: Microservices and containerization Amazon ECR: Image storage, scanning, lifecycle policies Amazon ECS \u0026amp; EKS: Deployment strategies, scaling, and orchestration AWS App Runner: Simplified container deployment Demo \u0026amp; Case Study: Microservices deployment comparison Key takeaway: Mastered containerization knowledge, understood differences between ECS and EKS, learned container image management with ECR, and experienced real microservices deployment.\n14:30 ‚Äì 14:45 | Break 14:45 ‚Äì 16:00 | Monitoring \u0026amp; Observability Main content:\nCloudWatch: Metrics, logs, alarms, and dashboards AWS X-Ray: Distributed tracing and performance insights Demo: Comprehensive observability setup Best Practices: Alerting, dashboards, and on-call processes Key takeaway: Learned how to set up effective monitoring and observability, understood how to use CloudWatch and X-Ray to monitor and debug applications, and applied best practices in alerting.\n16:00 ‚Äì 16:45 | DevOps Best Practices \u0026amp; Case Studies Main content:\nDeployment strategies: Feature flags, A/B testing Automated testing and CI/CD integration Incident management and postmortems Case Studies: DevOps transformation at startups and enterprises Key takeaway: Learned advanced deployment strategies, how to integrate automated testing, professional incident management processes, and real-world experience from case studies.\n16:45 ‚Äì 17:00 | Q\u0026amp;A \u0026amp; Wrap-up Main content:\nDevOps career pathways and professional opportunities AWS certification roadmap for DevOps Engineers Open discussion and Q\u0026amp;A session Key Takeaways Comprehensive understanding of DevOps culture and its implementation on AWS Proficient in building CI/CD pipeline with AWS CodeCommit, CodeBuild, CodeDeploy, CodePipeline Mastery of Infrastructure as Code with CloudFormation and CDK Deep understanding of container services: ECR, ECS, EKS, App Runner Learned to set up monitoring and observability with CloudWatch and X-Ray Applied DevOps best practices: deployment strategies, automated testing, incident management Real-world experience from case studies and demos Work Applications Build CI/CD pipeline for current projects Apply Infrastructure as Code to manage infrastructure Implement containerization for microservices architecture Set up effective monitoring and alerting system Apply deployment strategies like Blue/Green, Canary deployment Improve incident management and postmortem processes Event Experience Participating in \u0026ldquo;DevOps on AWS\u0026rdquo; was an extremely valuable experience, helping me gain deeper understanding of how to implement DevOps practices in AWS cloud environment.\nHighlights Learning from AWS experts: Gained access to knowledge from experienced Solution Architects, understanding DevOps mindset and practical applications. Detailed hands-on demos: Directly observed the complete CI/CD pipeline construction process, Infrastructure as Code demos with CloudFormation and CDK. Real-world case studies: Learned from successful DevOps implementation cases at both startups and enterprises. Quality networking: Opportunities to discuss with DevOps Engineers and Cloud Architects, sharing experiences and workplace challenges. Important Lessons DevOps is a journey, not a destination - need to apply step by step and continuously improve Infrastructure as Code is an important foundation for scalable DevOps Monitoring and observability are key to maintaining system reliability Container technology is the future of application deployment Event Images This event not only provided technical knowledge but also helped me understand the importance of DevOps culture in improving collaboration between Development and Operations teams, thereby enhancing product quality and delivery speed.\n"},{"uri":"https://thanhtu18.github.io/workshop-template/1-worklog/1.2-week2/","title":"Week 2 Worklog","tags":[],"description":"","content":"Week 2 Objectives: Set up Hybrid DNS with Route 53 Resolver. Set up VPC peering Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 1 - Deploying Amazon EC2 Instances + Create EC2 Server + Test Connection + Create NAT Gateway + Using Reachability Analyzer + Create EC2 Instance Connect Endpoint + AWS Systems Manager Session Manager + CloudWatch Monitoring \u0026amp; Alerting - Setting Up Site-to-Site VPN Connection in AWS + Create Virtual Private Gateway + Create Customer Gateway + Create VPN Connection + Customer Gateway Configuration + Modify AWS VPN Tunnel + Alternative VPN Configurations + VPN Troubleshooting Guide 15/09/2025 15/09/2025 https://000003.awsstudygroup.com/ 2 - VPN Connection using Strongswan with Transit Gateway + Create Transit Gateway + Create Transit Gateway Attachment + Configure Route Tables + Configure Customer Gateway - Clean up resources - Set up Hybrid DNS with Route 53 Resolver - Introduction Amazon Route 53 16/09/2025 16/09/2025 https://000003.awsstudygroup.com/ https://000004.awsstudygroup.com/ 3 - Preparation Amazon Route 53. + Generate Key Pair + Initialize CloudFormation Template + Configuring Security Group - Connecting to RDGW - Microsoft AD Deployment - Setup DNS + Create Route 53 Outbound Endpoint + Create Route 53 Resolver Rules + Create Route 53 Inbound Endpoints + Test results - Clean up resources 17/09/2025 17/09/2025 https://000004.awsstudygroup.com/ 4 - Setting up VPC Peering - Introduction - Prerequisites + Initialize CloudFormation Template + Create Security Group + Create EC2 Instance - Update Network ACL - VPC Peering - Route Tables - Cross-Peer DNS - Cleanup 18/09/2025 18/09/2025 https://000019.awsstudygroup.com/ 5 - Practice 1: Hands-on Creating and Configuring EC2 Server - Practice 2: Testing Connection to EC2 Instance - Practice 3: Setting Up Hybrid DNS with Route 53 Resolver - Practice 4: Exploring Amazon Route 53 and Its Features - Practice 5: Generating Key Pair and Testing Availability for Amazon Route 53 19/09/2025 19/09/2025 üèÜ Week 2 Achievements Deployed Amazon EC2 Instances\nCreated EC2 Server and tested connection Configured NAT Gateway, Reachability Analyzer, and EC2 Instance Connect Endpoint Enabled CloudWatch Monitoring \u0026amp; Alerting Used AWS Systems Manager Session Manager Configured Site-to-Site VPN Connection\nCreated Virtual Private Gateway, Customer Gateway, and VPN Connection Modified VPN Tunnel and performed troubleshooting Built VPN Connection using Strongswan with Transit Gateway\nCreated Transit Gateway and Attachments Configured Route Tables and Customer Gateway Set up Hybrid DNS with Route 53 Resolver Deployed Microsoft AD and Configured Route 53\nCreated Route 53 Endpoints (Inbound \u0026amp; Outbound) and Resolver Rules Tested DNS resolution and verified connectivity Implemented VPC Peering and Cross-Peer DNS\nCreated EC2 Instances, updated Network ACLs, and configured Route Tables Validated DNS across peered VPCs Completed Hands-on Practices\nEC2 setup and connection testing Hybrid DNS and Route 53 configuration Key pair generation and system validation "},{"uri":"https://thanhtu18.github.io/workshop-template/5-workshop/5.8-cognito/5.8.2-password-policies/","title":"Configure Password Policies","tags":[],"description":"","content":"Configure Password Policies In this step, you will configure password policies to ensure strong password requirements for your users.\nNavigate to your User Pool in the Cognito console Go to Authentication methods tab Click Edit in the Password policy section Password Policy Configuration Configure the following password requirements:\nPassword length:\nMinimum length: 8 characters Maximum length: 256 characters Password complexity:\n‚úÖ Require numbers ‚úÖ Require special characters ‚úÖ Require uppercase letters ‚úÖ Require lowercase letters Save Configuration Review your password policy settings Click Save changes Verify Password Policy The password policy is now active. Users will need to create passwords that meet these requirements:\nExample of valid passwords:\nMySecurePass123! StrongPassword2024# TaskManager@2025 Example of invalid passwords:\npassword (no uppercase, numbers, special chars) 12345678 (no letters, special chars) Pass1! (too short) "},{"uri":"https://thanhtu18.github.io/workshop-template/4-eventparticipated/","title":"Events Participated","tags":[],"description":"","content":"During my internship, I participated in three important events. Each one was a memorable experience that provided new, interesting, and useful knowledge, along with gifts and wonderful moments.\nEvent 1 Event Name: AI/ML/GenAI on AWS\nDate \u0026amp; Time: 09:00, November 15, 2025\nLocation: 26th Floor, Bitexco Tower, 02 Hai Trieu Street, Ben Nghe Ward, District 1, Ho Chi Minh City\nRole: Attendee\nContent Description: Comprehensive workshop on AI/ML/GenAI on AWS, including Amazon SageMaker, Foundation Models on Bedrock, Prompt Engineering, RAG, and building GenAI chatbots.\nValue Gained: Deep understanding of AWS AI/ML ecosystem, practical skills with SageMaker and Bedrock, ability to apply AI/GenAI to real-world projects.\nEvent 2 Event Name: AWS Cloud Mastery Series #2 - DevOps on AWS\nDate \u0026amp; Time: 09:00, November 17, 2025\nLocation: 26th Floor, Bitexco Tower, 02 Hai Trieu Street, Ben Nghe Ward, District 1, Ho Chi Minh City\nRole: Attendee\nContent Description: Event focused on DevOps practices on AWS, including CI/CD pipelines, Infrastructure as Code, Container services, and Monitoring \u0026amp; Observability.\nValue Gained: Mastery of DevOps culture and implementation on AWS, proficiency in CI/CD with CodeCommit/CodeBuild/CodeDeploy/CodePipeline, deep understanding of IaC and container orchestration.\nEvent 3 Event Name: AWS Cloud Mastery Series #3 - Well-Architected Security Pillar\nDate \u0026amp; Time: 08:30, November 29, 2025\nLocation: 26th Floor, Bitexco Financial Tower, 02 Hai Trieu Street, Ben Nghe Ward, District 1, Ho Chi Minh City\nRole: Attendee\nContent Description: In-depth workshop on the 5 security pillars of AWS Well-Architected Framework: Identity \u0026amp; Access Management, Detection, Infrastructure Protection, Data Protection, and Incident Response.\nValue Gained: Comprehensive understanding of cloud security architecture, mastery of modern IAM practices with zero-trust principles, skills to implement comprehensive monitoring and automated incident response.\n"},{"uri":"https://thanhtu18.github.io/workshop-template/5-workshop/5.2-prerequiste/","title":"Prerequiste","tags":[],"description":"","content":"IAM permissions Add the following IAM permission policy to your user account to deploy and cleanup this workshop.\n{ \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Sid\u0026#34;: \u0026#34;S3FullAccess\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;s3:CreateBucket\u0026#34;, \u0026#34;s3:DeleteBucket\u0026#34;, \u0026#34;s3:ListBucket\u0026#34;, \u0026#34;s3:GetBucketLocation\u0026#34;, \u0026#34;s3:GetBucketPolicy\u0026#34;, \u0026#34;s3:PutBucketPolicy\u0026#34;, \u0026#34;s3:DeleteBucketPolicy\u0026#34;, \u0026#34;s3:GetBucketCORS\u0026#34;, \u0026#34;s3:PutBucketCORS\u0026#34;, \u0026#34;s3:GetBucketWebsite\u0026#34;, \u0026#34;s3:PutBucketWebsite\u0026#34;, \u0026#34;s3:GetObject\u0026#34;, \u0026#34;s3:PutObject\u0026#34;, \u0026#34;s3:DeleteObject\u0026#34;, \u0026#34;s3:GetObjectVersion\u0026#34;, \u0026#34;s3:ListBucketVersions\u0026#34;, \u0026#34;s3:PutBucketVersioning\u0026#34;, \u0026#34;s3:GetBucketVersioning\u0026#34;, \u0026#34;s3:PutBucketPublicAccessBlock\u0026#34;, \u0026#34;s3:GetBucketPublicAccessBlock\u0026#34; ], \u0026#34;Resource\u0026#34;: [ \u0026#34;arn:aws:s3:::*\u0026#34; ] }, { \u0026#34;Sid\u0026#34;: \u0026#34;CloudFrontFullAccess\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;cloudfront:CreateDistribution\u0026#34;, \u0026#34;cloudfront:GetDistribution\u0026#34;, \u0026#34;cloudfront:GetDistributionConfig\u0026#34;, \u0026#34;cloudfront:UpdateDistribution\u0026#34;, \u0026#34;cloudfront:DeleteDistribution\u0026#34;, \u0026#34;cloudfront:ListDistributions\u0026#34;, \u0026#34;cloudfront:CreateInvalidation\u0026#34;, \u0026#34;cloudfront:GetInvalidation\u0026#34;, \u0026#34;cloudfront:ListInvalidations\u0026#34;, \u0026#34;cloudfront:CreateOriginAccessControl\u0026#34;, \u0026#34;cloudfront:GetOriginAccessControl\u0026#34;, \u0026#34;cloudfront:UpdateOriginAccessControl\u0026#34;, \u0026#34;cloudfront:DeleteOriginAccessControl\u0026#34;, \u0026#34;cloudfront:ListOriginAccessControls\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; }, { \u0026#34;Sid\u0026#34;: \u0026#34;WAFAndShieldAccess\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;wafv2:CreateWebACL\u0026#34;, \u0026#34;wafv2:GetWebACL\u0026#34;, \u0026#34;wafv2:UpdateWebACL\u0026#34;, \u0026#34;wafv2:DeleteWebACL\u0026#34;, \u0026#34;wafv2:ListWebACLs\u0026#34;, \u0026#34;wafv2:AssociateWebACL\u0026#34;, \u0026#34;wafv2:DisassociateWebACL\u0026#34;, \u0026#34;wafv2:CreateIPSet\u0026#34;, \u0026#34;wafv2:GetIPSet\u0026#34;, \u0026#34;wafv2:UpdateIPSet\u0026#34;, \u0026#34;wafv2:DeleteIPSet\u0026#34;, \u0026#34;wafv2:ListIPSets\u0026#34;, \u0026#34;wafv2:CreateRuleGroup\u0026#34;, \u0026#34;wafv2:GetRuleGroup\u0026#34;, \u0026#34;wafv2:UpdateRuleGroup\u0026#34;, \u0026#34;wafv2:DeleteRuleGroup\u0026#34;, \u0026#34;wafv2:ListRuleGroups\u0026#34;, \u0026#34;shield:DescribeSubscription\u0026#34;, \u0026#34;shield:GetSubscriptionState\u0026#34;, \u0026#34;shield:DescribeProtection\u0026#34;, \u0026#34;shield:ListProtections\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; }, { \u0026#34;Sid\u0026#34;: \u0026#34;CognitoFullAccess\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;cognito-idp:CreateUserPool\u0026#34;, \u0026#34;cognito-idp:DeleteUserPool\u0026#34;, \u0026#34;cognito-idp:DescribeUserPool\u0026#34;, \u0026#34;cognito-idp:ListUserPools\u0026#34;, \u0026#34;cognito-idp:UpdateUserPool\u0026#34;, \u0026#34;cognito-idp:CreateUserPoolClient\u0026#34;, \u0026#34;cognito-idp:DeleteUserPoolClient\u0026#34;, \u0026#34;cognito-idp:DescribeUserPoolClient\u0026#34;, \u0026#34;cognito-idp:UpdateUserPoolClient\u0026#34;, \u0026#34;cognito-idp:ListUserPoolClients\u0026#34;, \u0026#34;cognito-idp:CreateUserPoolDomain\u0026#34;, \u0026#34;cognito-idp:DeleteUserPoolDomain\u0026#34;, \u0026#34;cognito-idp:DescribeUserPoolDomain\u0026#34;, \u0026#34;cognito-idp:AdminCreateUser\u0026#34;, \u0026#34;cognito-idp:AdminDeleteUser\u0026#34;, \u0026#34;cognito-idp:AdminGetUser\u0026#34;, \u0026#34;cognito-idp:ListUsers\u0026#34;, \u0026#34;cognito-identity:CreateIdentityPool\u0026#34;, \u0026#34;cognito-identity:DeleteIdentityPool\u0026#34;, \u0026#34;cognito-identity:DescribeIdentityPool\u0026#34;, \u0026#34;cognito-identity:UpdateIdentityPool\u0026#34;, \u0026#34;cognito-identity:ListIdentityPools\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; }, { \u0026#34;Sid\u0026#34;: \u0026#34;APIGatewayFullAccess\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;apigateway:POST\u0026#34;, \u0026#34;apigateway:GET\u0026#34;, \u0026#34;apigateway:PUT\u0026#34;, \u0026#34;apigateway:PATCH\u0026#34;, \u0026#34;apigateway:DELETE\u0026#34;, \u0026#34;apigateway:UpdateRestApiPolicy\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; }, { \u0026#34;Sid\u0026#34;: \u0026#34;LambdaFullAccess\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;lambda:CreateFunction\u0026#34;, \u0026#34;lambda:DeleteFunction\u0026#34;, \u0026#34;lambda:GetFunction\u0026#34;, \u0026#34;lambda:GetFunctionConfiguration\u0026#34;, \u0026#34;lambda:ListFunctions\u0026#34;, \u0026#34;lambda:UpdateFunctionCode\u0026#34;, \u0026#34;lambda:UpdateFunctionConfiguration\u0026#34;, \u0026#34;lambda:PublishVersion\u0026#34;, \u0026#34;lambda:CreateAlias\u0026#34;, \u0026#34;lambda:UpdateAlias\u0026#34;, \u0026#34;lambda:DeleteAlias\u0026#34;, \u0026#34;lambda:GetAlias\u0026#34;, \u0026#34;lambda:InvokeFunction\u0026#34;, \u0026#34;lambda:AddPermission\u0026#34;, \u0026#34;lambda:RemovePermission\u0026#34;, \u0026#34;lambda:GetPolicy\u0026#34;, \u0026#34;lambda:PutFunctionConcurrency\u0026#34;, \u0026#34;lambda:DeleteFunctionConcurrency\u0026#34;, \u0026#34;lambda:TagResource\u0026#34;, \u0026#34;lambda:UntagResource\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; }, { \u0026#34;Sid\u0026#34;: \u0026#34;DynamoDBFullAccess\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;dynamodb:CreateTable\u0026#34;, \u0026#34;dynamodb:DeleteTable\u0026#34;, \u0026#34;dynamodb:DescribeTable\u0026#34;, \u0026#34;dynamodb:ListTables\u0026#34;, \u0026#34;dynamodb:UpdateTable\u0026#34;, \u0026#34;dynamodb:PutItem\u0026#34;, \u0026#34;dynamodb:GetItem\u0026#34;, \u0026#34;dynamodb:DeleteItem\u0026#34;, \u0026#34;dynamodb:UpdateItem\u0026#34;, \u0026#34;dynamodb:Query\u0026#34;, \u0026#34;dynamodb:Scan\u0026#34;, \u0026#34;dynamodb:BatchGetItem\u0026#34;, \u0026#34;dynamodb:BatchWriteItem\u0026#34;, \u0026#34;dynamodb:DescribeTimeToLive\u0026#34;, \u0026#34;dynamodb:UpdateTimeToLive\u0026#34;, \u0026#34;dynamodb:DescribeContinuousBackups\u0026#34;, \u0026#34;dynamodb:UpdateContinuousBackups\u0026#34;, \u0026#34;dynamodb:TagResource\u0026#34;, \u0026#34;dynamodb:UntagResource\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; }, { \u0026#34;Sid\u0026#34;: \u0026#34;KMSAccess\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;kms:CreateKey\u0026#34;, \u0026#34;kms:CreateAlias\u0026#34;, \u0026#34;kms:DeleteAlias\u0026#34;, \u0026#34;kms:DescribeKey\u0026#34;, \u0026#34;kms:ListKeys\u0026#34;, \u0026#34;kms:ListAliases\u0026#34;, \u0026#34;kms:Encrypt\u0026#34;, \u0026#34;kms:Decrypt\u0026#34;, \u0026#34;kms:GenerateDataKey\u0026#34;, \u0026#34;kms:PutKeyPolicy\u0026#34;, \u0026#34;kms:GetKeyPolicy\u0026#34;, \u0026#34;kms:EnableKey\u0026#34;, \u0026#34;kms:DisableKey\u0026#34;, \u0026#34;kms:ScheduleKeyDeletion\u0026#34;, \u0026#34;kms:CancelKeyDeletion\u0026#34;, \u0026#34;kms:TagResource\u0026#34;, \u0026#34;kms:UntagResource\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; }, { \u0026#34;Sid\u0026#34;: \u0026#34;SecretsManagerAccess\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;secretsmanager:CreateSecret\u0026#34;, \u0026#34;secretsmanager:DeleteSecret\u0026#34;, \u0026#34;secretsmanager:DescribeSecret\u0026#34;, \u0026#34;secretsmanager:GetSecretValue\u0026#34;, \u0026#34;secretsmanager:PutSecretValue\u0026#34;, \u0026#34;secretsmanager:UpdateSecret\u0026#34;, \u0026#34;secretsmanager:ListSecrets\u0026#34;, \u0026#34;secretsmanager:TagResource\u0026#34;, \u0026#34;secretsmanager:UntagResource\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; }, { \u0026#34;Sid\u0026#34;: \u0026#34;CodePipelineAccess\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;codepipeline:CreatePipeline\u0026#34;, \u0026#34;codepipeline:DeletePipeline\u0026#34;, \u0026#34;codepipeline:GetPipeline\u0026#34;, \u0026#34;codepipeline:GetPipelineState\u0026#34;, \u0026#34;codepipeline:UpdatePipeline\u0026#34;, \u0026#34;codepipeline:ListPipelines\u0026#34;, \u0026#34;codepipeline:StartPipelineExecution\u0026#34;, \u0026#34;codepipeline:StopPipelineExecution\u0026#34;, \u0026#34;codepipeline:GetPipelineExecution\u0026#34;, \u0026#34;codepipeline:ListPipelineExecutions\u0026#34;, \u0026#34;codepipeline:TagResource\u0026#34;, \u0026#34;codepipeline:UntagResource\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; }, { \u0026#34;Sid\u0026#34;: \u0026#34;CodeBuildAccess\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;codebuild:CreateProject\u0026#34;, \u0026#34;codebuild:DeleteProject\u0026#34;, \u0026#34;codebuild:UpdateProject\u0026#34;, \u0026#34;codebuild:BatchGetProjects\u0026#34;, \u0026#34;codebuild:ListProjects\u0026#34;, \u0026#34;codebuild:StartBuild\u0026#34;, \u0026#34;codebuild:StopBuild\u0026#34;, \u0026#34;codebuild:BatchGetBuilds\u0026#34;, \u0026#34;codebuild:ListBuildsForProject\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; }, { \u0026#34;Sid\u0026#34;: \u0026#34;CodeGuruReviewerAccess\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;codeguru-reviewer:AssociateRepository\u0026#34;, \u0026#34;codeguru-reviewer:DescribeRepositoryAssociation\u0026#34;, \u0026#34;codeguru-reviewer:ListRepositoryAssociations\u0026#34;, \u0026#34;codeguru-reviewer:DisassociateRepository\u0026#34;, \u0026#34;codeguru-reviewer:DescribeCodeReview\u0026#34;, \u0026#34;codeguru-reviewer:ListCodeReviews\u0026#34;, \u0026#34;codeguru-reviewer:ListRecommendations\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; }, { \u0026#34;Sid\u0026#34;: \u0026#34;CloudFormationAccess\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;cloudformation:CreateStack\u0026#34;, \u0026#34;cloudformation:DeleteStack\u0026#34;, \u0026#34;cloudformation:DescribeStacks\u0026#34;, \u0026#34;cloudformation:UpdateStack\u0026#34;, \u0026#34;cloudformation:ListStacks\u0026#34;, \u0026#34;cloudformation:GetTemplate\u0026#34;, \u0026#34;cloudformation:ValidateTemplate\u0026#34;, \u0026#34;cloudformation:DescribeStackEvents\u0026#34;, \u0026#34;cloudformation:DescribeStackResources\u0026#34;, \u0026#34;cloudformation:ListStackResources\u0026#34;, \u0026#34;cloudformation:CreateChangeSet\u0026#34;, \u0026#34;cloudformation:DeleteChangeSet\u0026#34;, \u0026#34;cloudformation:DescribeChangeSet\u0026#34;, \u0026#34;cloudformation:ExecuteChangeSet\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; }, { \u0026#34;Sid\u0026#34;: \u0026#34;CloudWatchLogsAccess\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;logs:CreateLogGroup\u0026#34;, \u0026#34;logs:DeleteLogGroup\u0026#34;, \u0026#34;logs:DescribeLogGroups\u0026#34;, \u0026#34;logs:CreateLogStream\u0026#34;, \u0026#34;logs:DeleteLogStream\u0026#34;, \u0026#34;logs:DescribeLogStreams\u0026#34;, \u0026#34;logs:PutLogEvents\u0026#34;, \u0026#34;logs:GetLogEvents\u0026#34;, \u0026#34;logs:FilterLogEvents\u0026#34;, \u0026#34;logs:PutRetentionPolicy\u0026#34;, \u0026#34;logs:DeleteRetentionPolicy\u0026#34;, \u0026#34;logs:TagLogGroup\u0026#34;, \u0026#34;logs:UntagLogGroup\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; }, { \u0026#34;Sid\u0026#34;: \u0026#34;XRayAccess\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;xray:PutTraceSegments\u0026#34;, \u0026#34;xray:PutTelemetryRecords\u0026#34;, \u0026#34;xray:GetSamplingRules\u0026#34;, \u0026#34;xray:GetSamplingTargets\u0026#34;, \u0026#34;xray:GetServiceGraph\u0026#34;, \u0026#34;xray:GetTraceSummaries\u0026#34;, \u0026#34;xray:GetTraceGraph\u0026#34;, \u0026#34;xray:BatchGetTraces\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; }, { \u0026#34;Sid\u0026#34;: \u0026#34;SNSAccess\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;sns:CreateTopic\u0026#34;, \u0026#34;sns:DeleteTopic\u0026#34;, \u0026#34;sns:GetTopicAttributes\u0026#34;, \u0026#34;sns:SetTopicAttributes\u0026#34;, \u0026#34;sns:ListTopics\u0026#34;, \u0026#34;sns:Subscribe\u0026#34;, \u0026#34;sns:Unsubscribe\u0026#34;, \u0026#34;sns:ListSubscriptions\u0026#34;, \u0026#34;sns:ListSubscriptionsByTopic\u0026#34;, \u0026#34;sns:Publish\u0026#34;, \u0026#34;sns:TagResource\u0026#34;, \u0026#34;sns:UntagResource\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; }, { \u0026#34;Sid\u0026#34;: \u0026#34;IAMPassRoleForServices\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;iam:PassRole\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34;, \u0026#34;Condition\u0026#34;: { \u0026#34;StringEquals\u0026#34;: { \u0026#34;iam:PassedToService\u0026#34;: [ \u0026#34;lambda.amazonaws.com\u0026#34;, \u0026#34;apigateway.amazonaws.com\u0026#34;, \u0026#34;codepipeline.amazonaws.com\u0026#34;, \u0026#34;codebuild.amazonaws.com\u0026#34;, \u0026#34;cloudformation.amazonaws.com\u0026#34; ] } } }, { \u0026#34;Sid\u0026#34;: \u0026#34;IAMRoleManagement\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;iam:CreateRole\u0026#34;, \u0026#34;iam:DeleteRole\u0026#34;, \u0026#34;iam:GetRole\u0026#34;, \u0026#34;iam:UpdateRole\u0026#34;, \u0026#34;iam:AttachRolePolicy\u0026#34;, \u0026#34;iam:DetachRolePolicy\u0026#34;, \u0026#34;iam:PutRolePolicy\u0026#34;, \u0026#34;iam:DeleteRolePolicy\u0026#34;, \u0026#34;iam:GetRolePolicy\u0026#34;, \u0026#34;iam:ListRolePolicies\u0026#34;, \u0026#34;iam:ListAttachedRolePolicies\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; } ] } "},{"uri":"https://thanhtu18.github.io/workshop-template/3-blogstranslated/3.2-blog2/","title":"Blog 2","tags":[],"description":"","content":"Democratize data for timely decisions with text-to-SQL at Parcel Perform by Yudho Ahmad Diponegoro, Le Vy, and Jun Kai Loke | on July 09, 2025 | in Amazon Athena, Amazon Bedrock, Amazon Bedrock Knowledge Bases, Business Intelligence, Customer Solutions, Generative AI, Intermediate (200), Supply Chain\nThis post is co-written with Le Vy from Parcel Perform. Access to accurate data is often the true differentiator between excellent decisions and timely decisions. This becomes even more critical for customer-facing decisions and actions. A modern AI deployed correctly can help your organization simplify data access to make accurate and timely decisions for customer-facing business teams, while minimizing the undifferentiated heavy lifting that your data team has to do. In this post, we share how Parcel Perform, a leading AI Delivery Experience platform for global ecommerce businesses, has implemented such a solution.\nAccurate post-purchase deliveries tracking can be crucial for many ecommerce merchants. Parcel Perform provides an AI-driven, intelligent end-to-end data and delivery experience and software as a service (SaaS) system for ecommerce merchants. The system uses AWS services and state-of-the-art AI to process hundreds of millions of daily parcel delivery movement data and provide a unified tracking capability across couriers for the merchants, with emphasis on accuracy and simplicity.\nThe business team in Parcel Perform often needs access to data to answer questions related to merchants‚Äô parcel deliveries, such as ‚ÄúDid we see a spike in delivery delays last week? If so, in which transit facilities were this observed, and what was the primary cause of the issue?‚Äù Previously, the data team had to manually form the query and run it to fetch the data. With the new generative AI-powered text-to-SQL capability in Parcel Perform, the business team can self-serve their data needs by using an AI assistant interface. In this post, we discuss how Parcel Perform incorporated generative AI, data storage, and data access through AWS services to make timely decisions.\nData analytics architecture The solution starts with data ingestion, storage, and access. Parcel Perform adopts a data analytics architecture as shown in the following diagram.\nOne key data type in the Parcel Perform parcel monitoring application is the parcel event data, which can reach billions of rows. This includes the parcel‚Äôs shipment status change, location change, and much more. This day-to-day data from multiple business units lands in relational databases hosted on Amazon Relational Database Service (Amazon RDS).\nAlthough relational databases are suitable for rapid data ingestion and consumption from the application, a separate analytics stack is needed to handle analytics in a scalable and performant way without disrupting the main application. These analytics needs include answering aggregation queries from questions like ‚ÄúHow many parcels were delayed last week?‚Äù\nParcel Perform uses Amazon Simple Storage Service (Amazon S3) with a query engine provided by Amazon Athena to meet their analytics needs. With this approach, Parcel Perform benefits from cost-effective storage while still being able to run SQL queries as needed on the data through Athena, which is priced on usage.\nData in Amazon S3 is stored in Apache Iceberg data format that allows data updates, which is useful in this case because the parcel events sometimes get updated. It also supports partitioning for better performance Amazon S3 Tables, launched in late 2024, is a feature for managing Iceberg tables, and could also be an option for you.\nParcel Perform uses an Apache Kafka cluster managed by Amazon Managed Streaming for Apache Kafka (Amazon MSK) as a stream to transfer data from source to S3 bucket. Amazon MSK Connect with Debezium connector streams data using change data capture (CDC) from Amazon RDS to Amazon MSK.\nApache Flink, running on Amazon Elastic Kubernetes Service (Amazon EKS), processes the data streams from Amazon MSK. It writes this data to the S3 bucket in Iceberg format, and updates the data schema in AWS Glue Data Catalog. This data schema allows Athena to query the correct data in the S3 bucket.\nNow that you understand how data is ingested and stored, we\u0026rsquo;ll look at how data is consumed through a data serving assistant using generative AI for business teams at Parcel Perform.\nData-queryable AI agent The users of the data serving AI agent at Parcel Perform are customer-facing business team members who regularly query parcel event data to answer questions from ecommerce merchants about deliveries and support them proactively. The following screenshot shows the AI assistant UI experience, powered by text-to-SQL with generative AI.\nThis functionality helped the Parcel Perform team and their customers save time, which we discuss later in this post. In the following section, we present the architecture that powers this feature.\nText-to-SQL AI agent architecture The data serving AI assistant architecture in Parcel Perform is shown in the following diagram.\nThe AI assistant UI is supported by an application built with the FastAPI framework hosted on Amazon EKS. It is also fronted by an Application Load Balancer to allow for potential horizontal scalability.\nThe application uses LangGraph to orchestrate the workflow of large language model (LLM) calls, tool usage, and memory checkpointing. The graph uses multiple tools, including tools from the SQLDatabase Toolkit to automatically retrieve data schema through Athena. The graph also uses an Amazon Bedrock Knowledge Bases retriever to retrieve business information from a knowledge base. Parcel Perform uses Anthropic\u0026rsquo;s Claude models in Amazon Bedrock to generate SQL.\nAlthough the function of Athena as a query engine to query the parcel event data on Amazon S3 is clear, Parcel Perform still needs a knowledge base. In this use case, the SQL generation performs better when the LLM has more business contextual information to help interpret database fields and translate logistics terminology into data representations. This is better illustrated with the following two examples:\nParcel Perform‚Äôs data lake operations use specific codes c for create and u for update. When analyzing data, Parcel Perform sometimes needs to focus only on initial creation records, where operation code is equal to c. Because this business logic might not be inherent in the training of LLMs in general, Parcel Perform explicitly defines this in their business context.\nIn logistics terminology, transit time has specific industry conventions. It‚Äôs measured in days, and same-day deliveries are recorded as transit_time = 0. Although this is intuitive for logistics professionals, an LLM might incorrectly interpret a request like ‚ÄúGet me all shipments with same-day delivery‚Äù by usingWHERE transit_time = 1 instead of WHERE transit_time = 0 in the generated SQL.\nTherefore, each incoming question goes to a Retrieval Augmented Generation (RAG) workflow to find potentially relevant stored business information, to enrich the context. This mechanism helps provide the specific rules and interpretations that even advanced LLMs might not be able to derive from general training data.\nParcel Perform uses Amazon Bedrock Knowledge Bases as a managed solution for the RAG workflow. They ingest business context information by uploading files to Amazon S3. Amazon Bedrock Knowledge Bases processes the files, chunks them, uses embedding models to create vectors, and stores the vectors in a vector database so they can be searched. These steps are fully managed by Amazon Bedrock Knowledge Bases. Parcel Perform stores the vectors in Amazon OpenSearch Serverless as the chosen vector database to simplify infrastructure management.\nAmazon Bedrock Knowledge Bases provides the Retrieve API, which takes in an input (such as a question from the AI assistant), converts it into a vector embedding, searches for relevant chunks of business context information in the vector database, and returns the top relevant document chunks. It is integrated with the LangChain Amazon Bedrock Knowledge Bases retriever by calling the invoke method.\nThe next step involves invoking an AI agent with the supplied business contextual information and the SQL generation prompt. The prompt was inspired by a prompt in LangChain Hub. The following is a code snippet of the prompt:\nYou are an agent designed to interact with a SQL database. Given an input question, create a syntactically correct {dialect} query to run, then look at the results of the query and return the answer. Unless the user specifies a specific number of examples they wish to obtain, always limit your query to at most {top_k} results. Relevant context: {rag_context} You can order the results by a relevant column to return the most interesting examples in the database. Never query for all the columns from a specific table, only ask for the relevant columns given the question. You have access to tools for interacting with the database. - Only use the below tools. Only use the information returned by the below tools to construct your final answer. - DO NOT make any DML statements (INSERT, UPDATE, DELETE, DROP etc.) to the database. - To start querying for final answer you should ALWAYS look at the tables in the database to see what you can query. Do NOT skip this step. - Then you should query the schema of the most relevant tables The prompt sample is part of the initial instruction for the agent. The data schema is automatically inserted by the tools from the SQLDatabase Toolkit at a later step of this agentic workflow. The following steps occur after a user enters a question in the AI assistant UI:\nThe question triggers a LangGraph execution run.\nThe following processes happen in parallel: a. The graph fetches the database schema from Athena through SQLDatabase Toolkit. b. The graph passes the question to the Amazon Bedrock Knowledge Bases retriever and gets a list of relevant business information regarding the question.\nThe graph invokes an LLM using Amazon Bedrock by passing the question, the conversation context, data schema, and business context information. The result is the generated SQL.\nThe graph uses the SQLDatabase Toolkit again to run the SQL through Athena and receive data results.\nThe data output is passed into an LLM to generate the final response based on the initial question asked. Amazon Bedrock Guardrails is used as a safeguard to avoid inappropriate inputs and responses.\nThe final response is returned to the user through the AI assistant UI.\nThe following diagram illustrates these steps.\nThis implementation demonstrates how Parcel Perform transforms raw inquiries into actionable data for timely decision-making. Security is also implemented in multiple components. From a network perspective, the EKS pods are placed in private subnets in Amazon Virtual Private Cloud (Amazon VPC) to improve network security of the AI assistant application. This AI agent is placed behind a backend layer that requires authentication. For data security, sensitive data is masked at rest in the S3 bucket. Parcel Perform also limits the permissions of the AWS Identity and Access Management (IAM) role used to access the S3 bucket so it can only access certain tables.\nIn the following sections, we discuss how Parcel Perform approached building this data transformation solution.\nFrom idea to production Parcel Perform started with the idea of freeing their data team from manually serving the request from the business team, while also improving the timeliness of the data availability to support the business team‚Äôs decision-making.\nWith the help of the AWS Solutions Architect team, Parcel Perform completed a proof of concept using AWS services and a Jupyter notebook in Amazon SageMaker Studio. After an initial success, Parcel Perform integrated the solution with their orchestration tool of choice, LangGraph.\nBefore going into production, Parcel Perform conducted thorough testing to verify result consistency. They added LangSmith Tracing to record the steps and results of the AI agent to evaluate its performance.\nThe Parcel Perform team discovered challenges during their journey, which we discuss in the following section. They performed prompt engineering to address those challenges. Eventually, the AI agent was integrated into production to be used by the business team. Afterward, Parcel Perform collected user feedback internally and monitored logs from LangSmith Tracing to verify performance was maintained.\nChallenges This journey was not immune to challenges.\nThis journey isn‚Äôt free from challenges. Firstly, some ecommerce merchants might have several records in the data lake under various names. For example, a merchant with the name ‚ÄúABC‚Äù might have multiple records such, as ‚ÄúABC Singapore Holdings Pte. Ltd.,‚Äù ‚ÄúABC Demo Account,‚Äù ‚ÄúABC Test Group,‚Äù and so on. For a question like ‚ÄúWas there any parcel shipment delay by ABC last week?‚Äù, the generated SQL has the element of WHERE merchant_name LIKE '%ABC%' which might result in ambiguity. During the proof of concept stage, this problem caused incorrect matching of the result.\nFor this challenge, Parcel Perform relies on careful prompt engineering to instruct the LLM to identify when the name was potentially ambiguous. The AI agent then calls Athena again to look for matching names. The LLM decides which merchant name to use based on multiple factors, including the significance in data volume contribution and the account status in the data lake. In the future, Parcel Perform intends to implement a more sophisticated technique by prompting the user to resolve the ambiguity.\nThe second challenge is about unrestricted questions that might yield expensive queries running across large amounts of data and resulting in longer query waiting time. Some of these questions might not have a LIMIT clause imposed in the query. To solve this, Parcel Perform instructs the LLM to add a LIMIT clause with a certain number of maximum results if the user doesn‚Äôt specify the intended number of results. In the future, Parcel Perform plans to use the query EXPLAIN results to identify heavy queries.\nThe third challenge is related to tracking usage and incurred cost of this particular solution. Having started multiple generative AI projects using Amazon Bedrock and sometimes with the same LLM ID, Parcel Perform must distinguish usage incurred by projects. Parcel Perform creates an inference profile for each project, associates the profile with tags, and includes that profile in each LLM call for that project. With this setup, Parcel Perform is able to segregate costs based on projects to improve cost visibility and monitoring.\nThe impact To extract data, the business team clarifies details with the data team, makes a request, checks feasibility, and waits for bandwidth. This process lengthens when requirements come from customers or teams in different time zones, with each clarification adding 12‚Äì24 hours due to asynchronous communication. Simpler requests made early in the workday might complete within 24 hours, whereas more complex requests or those during busy periods can take 3‚Äì5 business days.\nWith the text-to-SQL AI agent, this process is dramatically streamlined‚Äîminimizing the back-and-forth communication for requirement clarification, removing the dependency on data team bandwidth, and automating result interpretation.\nParcel Perform‚Äôs measurements show that the text-to-SQL AI agent reduces the average time-to-insight by 99%, from 2.3 days to an average of 10 minutes, saving approximately 3,850 total hours of wait time per month across requesters while maintaining data accuracy.\nUsers can directly query the data without intermediaries, receiving results in minutes rather than days. Teams across time zones can now access insights any time of day, alleviating the frustrating ‚Äúwait until Asia wakes up‚Äù or ‚Äúcatch EMEA before they leave‚Äù delays, leading to happier customers and faster problem-solving.\nThis transformation has profoundly impacted the data analytics team‚Äôs capacity and focus, freeing the data team for more strategic work and helping everyone make faster, more informed decisions. Before, the analysts spent approximately 25% of their working hours handling routine data extraction requests‚Äîequivalent to over 260 hours monthly across the team. Now, with basic and intermediate queries automated, this number has dropped to just 10%, freeing up nearly 160 hours each month for high-impact work. Analysts now focus on complex data analysis rather than spending time on basic data retrieval tasks.\nConclusion Parcel Perform‚Äôs solution demonstrates how you can use generative AI to enhance productivity and customer experience. Parcel Perform has built a text-to-SQL AI agent that transforms a business team‚Äôs question into SQL that can fetch the actual data. This improves the timeliness of data availability for decision-making that involves customers. Furthermore, the data team can avoid the undifferentiated heavy lifting to focus on complex data analysis tasks.\nThis solution uses multiple AWS services like Amazon Bedrock and tools like LangGraph. You can start with a proof of concept and consult your AWS Solutions Architect or engage with AWS Partners. If you have questions, post them on AWS re:Post. You can also make the development more straightforward with the help of Amazon Q Developer. When you face challenges, you can iterate to find the solution, which might include prompt engineering or adding additional steps to your workflow.\nSecurity is a top priority. Make sure your AI assistant has proper guardrails in place to protect against prompt threats, inappropriate topics, profanity, leaked data, and other security issues. You can integrate Amazon Bedrock Guardrails with your generative AI application through an API.To learn more, refer to the following resources:\nBuild a robust text-to-SQL solution generating complex queries, self-correcting, and querying diverse data sources X√¢y d·ª±ng gi·∫£i ph√°p chuy·ªÉn ƒë·ªïi vƒÉn b·∫£n sang SQL m·∫°nh m·∫Ω, t·∫°o ra c√°c truy v·∫•n ph·ª©c t·∫°p, t·ª± ƒë·ªông s·ª≠a l·ªói v√† truy v·∫•n c√°c ngu·ªìn d·ªØ li·ªáu ƒëa d·∫°ng LangGraph agents with Amazon Bedrock workshop Build a knowledge base by connecting to a structured data store About the authors Yudho Ahmad Diponegoro is a Senior Solutions Architect at AWS. Having been part of Amazon for 10+ years, he has had various roles from software development to solutions architecture. He helps startups in Singapore when it comes to architecting in the cloud. While he keeps his breadth of knowledge across technologies and industries, he focuses in AI and machine learning where he has been guiding various startups in ASEAN to adopt machine learning and generative AI at AWS.\nLe Vy is the AI Team Lead at Parcel Perform, where she drives the development of AI applications and explores emerging AI research. She started her career in data analysis and deepened her focus on AI through a Master‚Äôs in Artificial Intelligence. Passionate about applying data and AI to solve real business problems, she also dedicates time to mentoring aspiring technologists and building a supportive community for youth in tech. Through her work, Vy actively challenges gender norms in the industry and champions lifelong learning as a key to innovation.\nLoke Jun Kai is a GenAI/ML Specialist Solutions Architect in AWS, covering strategic customers across the ASEAN region. He works with customers ranging from Start-up to Enterprise to build cutting-edge use cases and scalable GenAI Platforms. His passion in the AI space, constant research and reading, have led to many innovative solutions built with concrete business outcomes. Outside of work, he enjoys a good game of tennis and chess.\n"},{"uri":"https://thanhtu18.github.io/workshop-template/4-eventparticipated/4.3-event3/","title":"Event 3","tags":[],"description":"","content":"Summary Report: \u0026ldquo;AWS Cloud Mastery Series #3 - Well-Architected Security Pillar\u0026rdquo; Event Objectives Master the 5 core pillars of AWS Well-Architected Security Framework Understand modern security threats and best practices in cloud environment Learn comprehensive security implementation from identity management to incident response Apply security principles in real-world enterprise scenarios Event Agenda \u0026amp; Highlights 8:30 ‚Äì 8:50 AM | Opening \u0026amp; Security Foundation Main content:\nRole of Security Pillar in Well-Architected Framework Core principles: Least Privilege ‚Äì Zero Trust ‚Äì Defense in Depth Shared Responsibility Model understanding Top cloud security threats in Vietnam\u0026rsquo;s enterprise environment Key takeaway: Gained foundational understanding of AWS security philosophy and how shared responsibility model applies to different service layers. Learned about current security landscape and common vulnerabilities in Vietnamese cloud environments.\n8:50 ‚Äì 9:30 AM | Pillar 1 ‚Äî Identity \u0026amp; Access Management Main content:\nModern IAM Architecture: Users, Roles, Policies best practices Avoiding long-term credentials and implementing temporary access IAM Identity Center: SSO implementation and permission sets SCP \u0026amp; permission boundaries for multi-account governance MFA implementation, credential rotation strategies, Access Analyzer utilization Mini Demo: IAM Policy validation and access simulation Key takeaway: Mastered modern IAM patterns, understood the critical importance of eliminating long-term credentials, and gained hands-on experience with policy validation tools. Learned multi-account security governance strategies.\n9:30 ‚Äì 9:55 AM | Pillar 2 ‚Äî Detection \u0026amp; Continuous Monitoring Main content:\nCloudTrail organization-level logging and management GuardDuty threat detection and Security Hub centralization Comprehensive logging: VPC Flow Logs, ALB logs, S3 access logs Alerting and automation with EventBridge integration Detection-as-Code approach for infrastructure and security rules Key takeaway: Understood comprehensive monitoring strategy across all AWS layers, learned how to implement automated threat detection and response, and gained insight into treating security rules as code for better governance.\n9:55 ‚Äì 10:10 AM | Coffee Break Networking with security professionals and AWS experts.\n10:10 ‚Äì 10:40 AM | Pillar 3 ‚Äî Infrastructure Protection Main content:\nVPC segmentation strategies and network isolation Private vs public subnet placement best practices Security Groups vs NACLs: practical application models WAF + Shield + Network Firewall integration Workload protection: EC2, ECS/EKS security fundamentals Key takeaway: Mastered network security layering approach, understood when to use different network security controls, and learned workload-specific security implementations for containers and compute services.\n10:40 ‚Äì 11:10 AM | Pillar 4 ‚Äî Data Protection Main content:\nKMS: key policies, grants, and automated rotation Encryption at-rest \u0026amp; in-transit: S3, EBS, RDS, DynamoDB implementation Secrets Manager \u0026amp; Parameter Store rotation patterns Data classification frameworks and access guardrails Key takeaway: Deep understanding of AWS encryption services, learned practical patterns for secrets management and rotation, and understood how to implement data classification and protection strategies across different service types.\n11:10 ‚Äì 11:40 AM | Pillar 5 ‚Äî Incident Response Main content:\nIR lifecycle according to AWS best practices Practical playbooks for common scenarios: Compromised IAM credentials S3 public exposure incidents EC2 malware detection and response Snapshot creation, workload isolation, evidence collection procedures Automated response implementation using Lambda and Step Functions Key takeaway: Gained practical incident response skills with AWS-specific tools, learned automated response patterns, and understood how to implement security orchestration for faster incident resolution.\n11:40 ‚Äì 12:00 PM | Wrap-Up \u0026amp; Q\u0026amp;A Main content:\nComprehensive review of all 5 security pillars Common pitfalls and real-world challenges in Vietnamese enterprises Security learning roadmap: Security Specialty and Solutions Architect Pro certifications Key Takeaways Comprehensive Security Framework: Deep understanding of all 5 Well-Architected Security Pillars and their interconnections Modern IAM Practices: Mastery of identity and access management with zero-trust principles Detection \u0026amp; Response: Implementation of comprehensive monitoring and automated incident response Infrastructure Security: Network segmentation and workload protection strategies Data Protection: Encryption and secrets management best practices Practical Application: Real-world security scenarios and enterprise-specific challenges Work Applications Implement comprehensive security assessment using Well-Architected Security Pillar framework Redesign IAM architecture following modern best practices and eliminating long-term credentials Set up automated threat detection and response using GuardDuty, Security Hub, and EventBridge Implement network segmentation and workload protection for current infrastructure Establish data classification and encryption standards across all services Develop incident response playbooks and automation for common security scenarios Event Experience Participating in \u0026ldquo;AWS Cloud Mastery Series #3\u0026rdquo; on Well-Architected Security Pillar was an invaluable experience that provided comprehensive understanding of cloud security best practices and real-world implementation strategies.\nHighlights Expert-led Security Training: Learning from AWS security specialists with deep enterprise experience Hands-on Demonstrations: Practical demos of IAM policy validation, threat detection, and incident response Real-world Case Studies: Learning from actual security incidents and response scenarios Comprehensive Coverage: All 5 security pillars covered with practical implementation guidance Important Lessons Security is not a one-time implementation but a continuous process requiring constant monitoring and improvement Zero-trust architecture requires fundamental changes in how we approach identity and access management Automation is critical for effective security at scale, especially for detection and incident response Data protection must be implemented at every layer, from infrastructure to application level Event Images This security-focused event significantly enhanced my understanding of cloud security architecture and provided practical tools for implementing enterprise-grade security in AWS environments. The comprehensive coverage of all security pillars and hands-on approach made complex security concepts accessible and immediately applicable.\n"},{"uri":"https://thanhtu18.github.io/workshop-template/1-worklog/1.3-week3/","title":"Week 3 Worklog","tags":[],"description":"","content":"Week 3 Objectives: Set up AWS Transit Gateway Create Transit Gateway Attachments and Route Tables Learn Amazon EC2 comprehensive concepts and services Study EC2 Auto Scaling, EFS/FSx, Lightsail, and MGN Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 1 - Introduction to AWS Transit Gateway + Understanding Transit Gateway concepts + VPC Peering vs Transit Gateway comparison + Transit Gateway benefits and use cases + Review lab architecture and prerequisites 22/09/2025 22/09/2025 https://000020.awsstudygroup.com/ 2 - Create Transit Gateway + Configure Transit Gateway parameters + Set up Transit Gateway settings + Review Transit Gateway configuration 23/09/2025 23/09/2025 https://000020.awsstudygroup.com/ 3 - Create Transit Gateway Attachments + Attach VPCs to Transit Gateway + Configure attachment settings + Verify attachment status 24/09/2025 24/09/2025 https://000020.awsstudygroup.com/ 4 - Transit Gateway Route Tables \u0026amp; Testing + Create Transit Gateway Route Tables + Add Routes to VPC Route Tables + Test connectivity between VPCs + Resource cleanup 25/09/2025 25/09/2025 https://000020.awsstudygroup.com/ 5 - Module 03-01: Amazon EC2 Comprehensive Study + EC2 Instance Types + AMI / Backup / Key Pair + Elastic Block Store + Instance Store + User Data + Meta Data + EC2 Auto Scaling - EFS/FSx - Lightsail - MGN 26/09/2025 26/09/2025 üèÜ Week 3 Achievements Understand AWS Transit Gateway Concepts\nLearn Transit Gateway vs VPC Peering Understand the Benefits of Scalable Network Architectures Review a lab architecture with four VPCs Properly Cleanup Transit Gateway Resources Configure Transit Gateway Attachments\nSuccessfully Connect Multiple VPCs to Transit Gateway Verify Attachment and Connection Status Establish VPC-to-Transit Gateway Connections Amazon EC2 Comprehensive Concepts (Day 5)\nLearned different EC2 instance families, types and sizing Understood AMI concepts, backup strategies and Key Pair management Studied EBS volume types, performance, snapshots and encryption Compared Instance Store vs EBS storage characteristics "},{"uri":"https://thanhtu18.github.io/workshop-template/5-workshop/5.3-lambda/","title":"Lambda","tags":[],"description":"","content":"Initializing Lambda Create Lambda Function Select Create function ‚Üí Author from scratch.\nIn the Basic information section, configure:\nFunction name: taskhub-backend-1 Runtime: .NET 8 (C#/F#/Powershell) Architecture: arm64 (default) In Permissions ‚Üí Change default execution role:\nSelect Create a new role with basic Lambda permissions\n(AWS will automatically create a role with CloudWatch Logs write access) Keep the remaining settings as default and click Create function to finish.\nBuild Backend \u0026amp; Package into ZIP Because Lambda does not allow direct code editing for the .NET runtime, you must upload a ZIP file.\nOn your local machine, build the backend using: dotnet publish -c Release -o publish Open the publish folder ‚Üí Select all files inside it, do not select the folder itself.\nCompress them into backend.zip Upload Source Code to Lambda In the Code tab ‚Üí select Upload from ‚Üí .zip file Select the file backend.zip\nClick Upload and wait for Lambda to deploy\nAfter the upload completes, the Code properties section will display the package size, SHA256 hash, and the last updated timestamp.\nVerify Runtime Settings In the Runtime settings section:\nRuntime: .NET 8 Handler: YourProject::YourNamespace.YourHandler::FunctionHandler\n(depends on your project structure) Architecture: arm64 Ensure that Lambda is running the correct entry point.\nConfigure Environment Variables Open the Configuration tab ‚Üí Environment variables Click Edit ‚Üí Add environment variable Add example variables:\nASPNETCORE_ENVIRONMENT = Production DynamoDB_TableName = Your_dynamoDB Jwt_Secret = Your_JWT_Secret Click Save.\nConfigure Timeout \u0026amp; Memory Open Configuration ‚Üí General configuration\nClick Edit\nSet:\nMemory: 512MB ‚Äì 1024MB Timeout: 30s (suitable for APIs) or higher if needed Click Save.\nAdd Trigger (API Gateway) To expose Lambda as an API with a public URL:\nGo to Configuration ‚Üí Triggers Click Add trigger Choose:\nAPI Gateway Create an API REST API Security: Open (or IAM/Authorizer depending on your system)\nClick Add\n"},{"uri":"https://thanhtu18.github.io/workshop-template/5-workshop/5.8-cognito/5.8.3-email-verification/","title":"Setup Email Verification","tags":[],"description":"","content":"Setup Email Verification In this step, you will configure email verification to ensure users verify their email addresses during registration.\nNavigate to your User Pool in the Cognito console Go to Sign-up experience tab Click Edit in the Attribute verification and user account confirmation section Configure Email Verification Attribute verification and user account confirmation:\n‚úÖ Send email message, verify email address Verification message: Code Email verification subject: Verify your email for Task Management System Email verification message: Your verification code for Task Management System is {####}. Please enter this code to complete your registration. Configure Email Delivery Email delivery method:\nSend email with Cognito (for development) FROM email address: no-reply@verificationemail.com Note: For production applications, consider using Amazon SES for better email deliverability and custom domains.\n"},{"uri":"https://thanhtu18.github.io/workshop-template/3-blogstranslated/3.3-blog3/","title":"Blog 3","tags":[],"description":"","content":"Build a scalable AI video generator using Amazon SageMaker AI and CogVideoX by Nick Biso, Jinzhao Feng, Katherine Feng, and Natasha Tchir | on JUN 19 2025 | in Amazon SageMaker, Amazon SageMaker AI, Artificial Intelligence, Generative AI,Intermediate (200)\nIn recent years, the rapid advancement of artificial intelligence and machine learning (AI/ML) technologies has revolutionized various aspects of digital content creation. One particularly exciting development is the emergence of video generation capabilities, which offer unprecedented opportunities for companies across diverse industries. This technology allows for the creation of short video clips that can be seamlessly combined to produce longer, more complex videos. The potential applications of this innovation are vast and far-reaching, promising to transform how businesses communicate, market, and engage with their audiences. Video generation technology presents a myriad of use cases for companies looking to enhance their visual content strategies. For instance, ecommerce businesses can use this technology to create dynamic product demonstrations, showcasing items from multiple angles and in various contexts without the need for extensive physical photoshoots. In the realm of education and training, organizations can generate instructional videos tailored to specific learning objectives, quickly updating content as needed without re-filming entire sequences. Marketing teams can craft personalized video advertisements at scale, targeting different demographics with customized messaging and visuals. Furthermore, the entertainment industry stands to benefit greatly, with the ability to rapidly prototype scenes, visualize concepts, and even assist in the creation of animated content. The flexibility offered by combining these generated clips into longer videos opens up even more possibilities. Companies can create modular content that can be quickly rearranged and repurposed for different displays, audiences, or campaigns. This adaptability not only saves time and resources, but also allows for more agile and responsive content strategies. As we delve deeper into the potential of video generation technology, it becomes clear that its value extends far beyond mere convenience, offering a transformative tool that can drive innovation, efficiency, and engagement across the corporate landscape.\nIn this post, we explore how to implement a robust AWS-based solution for video generation that uses the CogVideoX model and Amazon SageMaker AI.\nSolution overview Our architecture delivers a highly scalable and secure video generation solution using AWS managed services. The data management layer implements three purpose-specific Amazon Simple Storage Service (Amazon S3) buckets‚Äîfor input videos, processed outputs, and access logging‚Äîeach configured with appropriate encryption and lifecycle policies to support data security throughout its lifecycle.\nFor compute resources, we use AWS Fargate for Amazon Elastic Container Service (Amazon ECS) to host the Streamlit web application, providing serverless container management with automatic scaling capabilities. Traffic is efficiently distributed through an Application Load Balancer. The AI processing pipeline uses SageMaker AI processing jobs to handle video generation tasks, decoupling intensive computation from the web interface for cost optimization and enhanced maintainability. User prompts are refined through Amazon Bedrock, which feeds into the CogVideoX-5b model for high-quality video generation, creating an end-to-end solution that balances performance, security, and cost-efficiency.\nThe following diagram illustrates the solution architecture.\nSolution overview CogVideoX is an open source, state-of-the-art text-to-video generation model capable of producing 10-second continuous videos at 16 frames per second with a resolution of 768√ó1360 pixels. The model effectively translates text prompts into coherent video narratives, addressing common limitations in previous video generation systems.\nThe model uses three key innovations:\nA 3D Variational Autoencoder (VAE) that compresses videos along both spatial and temporal dimensions, improving compression efficiency and video quality An expert transformer with adaptive LayerNorm that enhances text-to-video alignment through deeper fusion between modalities Progressive training and multi-resolution frame pack techniques that enable the creation of longer, coherent videos with significant motion elements CogVideoX also benefits from an effective text-to-video data processing pipeline with various preprocessing strategies and a specialized video captioning method, contributing to higher generation quality and better semantic alignment. The model‚Äôs weights are publicly available, making it accessible for implementation in various business applications, such as product demonstrations and marketing content. The following diagram shows the architecture of the model. Prompt enhancement To improve the quality of video generation, the solution provides an option to enhance user-provided prompts. This is done by instructing a large language model (LLM), in this case Anthropic‚Äôs Claude, to take a user‚Äôs initial prompt and expand upon it with additional details, creating a more comprehensive description for video creation. The prompt consists of three parts:\nRole section ‚Äì Defines the AI‚Äôs purpose in enhancing prompts for video generation Task section ‚Äì Specifies the instructions needed to be performed with the original prompt Prompt section ‚Äì Where the user‚Äôs original input is inserted By adding more descriptive elements to the original prompt, this system aims to provide richer, more detailed instructions to video generation models, potentially resulting in more accurate and visually appealing video outputs. We use the following prompt template for this solution: \u0026lt;Role\u0026gt; Your role is to enhance the user prompt that is given to you by providing additional details to the prompt. The end goal is to covert the user prompt into a short video clip, so it is necessary to provide as much information you can. \u0026lt;/Role\u0026gt; \u0026lt;Task\u0026gt; You must add details to the user prompt in order to enhance it for video generation. You must provide a 1 paragraph response. No more and no less. Only include the enhanced prompt in your response. Do not include anything else. \u0026lt;/Task\u0026gt; \u0026lt;Prompt\u0026gt; {prompt} \u0026lt;/Prompt\u0026gt; Prerequisites Before you deploy the solution, make sure you have the following prerequisites:\nThe AWS CDK Toolkit ‚Äì Install the AWS CDK Toolkit globally using npm: npm install -g aws-cdk This provides the core functionality for deploying infrastructure as code to AWS. Docker Desktop ‚Äì This is required for local development and testing. It makes sure container images can be built and tested locally before deployment. The AWS CLI ‚Äì The AWS Command Line Interface (AWS CLI) must be installed and configured with appropriate credentials. This requires an AWS account with necessary permissions. Configure the AWS CLI using aws configure with your access key and secret. Python Environment ‚Äì You must have Python 3.11+ installed on your system. We recommend using a virtual environment for isolation. This is required for both the AWS CDK infrastructure and Streamlit application. Active AWS account ‚Äì You will need to raise a service quota request for SageMaker to ml.g5.4xlarge for processing jobs. Deploy the solution This solution has been tested in the us-east-1 AWS Region. Complete the following steps to deploy:\nCreate and activate a virtual environment: python -m venv .` venv source .venv/bin/activate Install infrastructure dependencies: cd infrastructure pip install -r requirements.txt Bootstrap the AWS CDK (if not already done in your AWS account): cdk bootstrap Deploy the infrastructure: cdk deploy -c allowed_ips=\u0026#39;[\u0026#34;\u0026#39;$(curl -s ifconfig.me)\u0026#39;/32\u0026#34;]\u0026#39; To access the Streamlit UI, choose the link for StreamlitURL in the AWS CDK output logs after deployment is successful. The following screenshot shows the Streamlit UI accessible through the URL.\nBasic video generation Complete the following steps to generate a video:\nInput your natural language prompt into the text box at the top of the page. Copy this prompt to the text box at the bottom. Choose Generate Video to create a video using this basic prompt. The following is the output from the simple prompt ‚ÄúA bee on a flower.‚Äù Enhanced video generation For higher-quality results, complete the following steps:\nEnter your initial prompt in the top text box. Choose Enhance Prompt to send your prompt to Amazon Bedrock. Wait for Amazon Bedrock to expand your prompt into a more descriptive version. Review the enhanced prompt that appears in the lower text box. Edit the prompt further if desired. Choose Generate Video to initiate the processing job with CogVideoX. When processing is complete, your video will appear on the page with a download option.The following is an example of an enhanced prompt and output:\n\u0026#34;\u0026#34;\u0026#34; A vibrant yellow and black honeybee gracefully lands on a large, blooming sunflower in a lush garden on a warm summer day. The bee\u0026#39;s fuzzy body and delicate wings are clearly visible as it moves methodically across the flower\u0026#39;s golden petals, collecting pollen. Sunlight filters through the petals, creating a soft, warm glow around the scene. The bee\u0026#39;s legs are coated in pollen as it works diligently, its antennae twitching occasionally. In the background, other colorful flowers sway gently in a light breeze, while the soft buzzing of nearby bees can be heard \u0026#34;\u0026#34;\u0026#34; Add an image to your prompt If you want to include an image with your text prompt, complete the following steps:\n1.Complete the text prompt and optional enhancement steps. 2. Choose Include an Image. 3. Upload the photo you want to use. 4. With both text and image now prepared, choose Generate Video to start the processing job.\nThe following is an example of the previous enhanced prompt with an included image.\nTo view more samples, check out the CogVideoX gallery.\nClean up To avoid incurring ongoing charges, clean up the resources you created as part of this post:\ncdk destroy\nConsiderations Although our current architecture serves as an effective proof of concept, several enhancements are recommended for a production environment. Considerations include implementing an API Gateway with AWS Lambda backed REST endpoints for improved interface and authentication, introducing a queue-based architecture using Amazon Simple Queue Service (Amazon SQS) for better job management and reliability, and enhancing error handling and monitoring capabilities.\nConclusion Video generation technology has emerged as a transformative force in digital content creation, as demonstrated by our comprehensive AWS-based solution using the CogVideoX model. By combining powerful AWS services like Fargate, SageMaker, and Amazon Bedrock with an innovative prompt enhancement system, we‚Äôve created a scalable and secure pipeline capable of producing high-quality video clips. The architecture‚Äôs ability to handle both text-to-video and image-to-video generation, coupled with its user-friendly Streamlit interface, makes it an invaluable tool for businesses across sectors‚Äîfrom ecommerce product demonstrations to personalized marketing campaigns. As showcased in our sample videos, the technology delivers impressive results that open new avenues for creative expression and efficient content production at scale. This solution represents not just a technological advancement, but a glimpse into the future of visual storytelling and digital communication.\nTo learn more about CogVideoX, refer to CogVideoX on Hugging Face. Try out the solution for yourself, and share your feedback in the comments.\nAbout the Authors Nick Bisois a Machine Learning Engineer at AWS Professional Services. He solves complex organizational and technical challenges using data science and engineering. In addition, he builds and deploys AI/ML models on the AWS Cloud. His passion extends to his proclivity for travel and diverse cultural experiences.\nNatasha Tchir is a Cloud Consultant at the Generative AI Innovation Center, specializing in machine learning. With a strong background in ML, she now focuses on the development of generative AI proof-of-concept solutions, driving innovation and applied research within the GenAIIC.\nKatherine Feng is a Cloud Consultant at AWS Professional Services within the Data and ML team. She has extensive experience building full-stack applications for AI/ML use cases and LLM-driven solutions.\nJinzhao Feng is a Machine Learning Engineer at AWS Professional Services. He focuses on architecting and implementing large-scale generative AI and classic ML pipeline solutions. He is specialized in FMOps, LLMOps, and distributed training.\n"},{"uri":"https://thanhtu18.github.io/workshop-template/1-worklog/1.4-week4/","title":"Week 4 Worklog","tags":[],"description":"","content":"Week 4 Objectives: Deploy AWS Backup to automate data protection Learn AWS Storage Gateway for hybrid cloud storage Start with Amazon S3 fundamentals and static website hosting Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 1 - Deploy AWS Backup to the System + Introduction to AWS Backup and SNS + Deploy the Infrastructure + Create Backup Plan for AWS resources 29/09/2025 29/09/2025 https://000013.awsstudygroup.com/ 2 - Complete AWS Backup Setup + Configure Notification Settings with SNS + Test Restore operations + Resource Cleanup 30/09/2025 30/09/2025 https://000013.awsstudygroup.com/ 3 - Using File Storage Gateway + Preparation and setup + Create Storage Gateway + Create File Shares + Mount File Sharing on On-premise Machine + Resource cleanup 01/10/2025 01/10/2025 https://000024.awsstudygroup.com/ 4 - Starting with Amazon S3 (Part 1) + Introduction to Amazon S3 + Preparation and setup + Enable Static website feature + Configure public access block + Configure public objects + Test website 02/10/2025 02/10/2025 https://000057.awsstudygroup.com/ 5 - Amazon S3 Advanced Features (Part 2) + Speed up Static website with CloudFront + Bucket Versioning + Move Objects + Replication Object multi Region + Notes \u0026amp; Best Practices 03/10/2025 03/10/2025 https://000057.awsstudygroup.com/ üèÜ Week 4 Achievements understand AWS Backup Service\nUunderstand AWS Backup for centralized data protection Created automated Backup Plans for AWS resources Configured backup policies for EBS, RDS, DynamoDB, and EFS Set up notification systems with AWS SNS Successfully tested backup and restore operations Amazon S3 Fundamentals and Static Website Hosting\nMastered Amazon S3 object storage concepts Configured static website hosting on S3 Set up public access blocks and object permissions Implemented CloudFront for website acceleration Learned S3 versioning and object lifecycle management Advanced S3 Features\nConfigured bucket versioning for data protection Implemented object movement and lifecycle policies Set up cross-region replication for disaster recovery Applied S3 best practices and security recommendations Uunderstand S3 storage classes and cost optimization "},{"uri":"https://thanhtu18.github.io/workshop-template/5-workshop/5.4-apigateway/","title":"API Gateway","tags":[],"description":"","content":"Initializing API Gateway Create an API Gateway Function Open API Gateway\nSelect Create API Choose API type: REST API\nThen select: Build\nConfigure API Information Enter API name, for example: taskhub-backend-api\nEndpoint type: Regional\nSecurity policy: SecurityPolicy_TLS13_1_3_2025_09\nClick Create API\nCreate Resource for API In the API Gateway menu, select Resources\nClick Create resource\nEnter:\nResource name: auth, task, projects ‚Ä¶ depending on the project Resource path: /auth, /task, ‚Ä¶ Click Create resource\nCreate Method and Connect to Lambda Select a Resource ‚Üí click Create method\nChoose ANY (or POST, GET depending on your API) Integration type: Lambda Function Tick Use Lambda Proxy integration Select Region Enter the name of the Lambda function, e.g., taskhub-backend_1 Click Save\nRepeat these steps to create additional API routes.\nGrant API Gateway Permission to Call Lambda Choose Deploy API\nCreate a new stage (if not available): prod1 Click Deploy Result: You will receive an Invoke URL like:\nhttps://ne6pw5hqej.execute-api.ap-southeast-1.amazonaws.com/prod1\n"},{"uri":"https://thanhtu18.github.io/workshop-template/3-blogstranslated/3.4-blog4/","title":"Blog 4","tags":[],"description":"","content":" ‚ö†Ô∏è Note: The information below is for reference purposes only. Please do not copy verbatim for your report, including this warning.\nGetting Started with Healthcare Data Lakes: Using Microservices Data lakes can help hospitals and healthcare facilities turn data into business insights, maintain business continuity, and protect patient privacy. A data lake is a centralized, managed, and secure repository to store all your data, both in its raw and processed forms for analysis. Data lakes allow you to break down data silos and combine different types of analytics to gain insights and make better business decisions.\nThis blog post is part of a larger series on getting started with setting up a healthcare data lake. In my final post of the series, ‚ÄúGetting Started with Healthcare Data Lakes: Diving into Amazon Cognito‚Äù, I focused on the specifics of using Amazon Cognito and Attribute Based Access Control (ABAC) to authenticate and authorize users in the healthcare data lake solution. In this blog, I detail how the solution evolved at a foundational level, including the design decisions I made and the additional features used. You can access the code samples for the solution in this Git repo for reference.\nArchitecture Guidance The main change since the last presentation of the overall architecture is the decomposition of a single service into a set of smaller services to improve maintainability and flexibility. Integrating a large volume of diverse healthcare data often requires specialized connectors for each format; by keeping them encapsulated separately as microservices, we can add, remove, and modify each connector without affecting the others. The microservices are loosely coupled via publish/subscribe messaging centered in what I call the ‚Äúpub/sub hub.‚Äù\nThis solution represents what I would consider another reasonable sprint iteration from my last post. The scope is still limited to the ingestion and basic parsing of HL7v2 messages formatted in Encoding Rules 7 (ER7) through a REST interface.\nThe solution architecture is now as follows:\nFigure 1. Overall architecture; colored boxes represent distinct services.\nWhile the term microservices has some inherent ambiguity, certain traits are common:\nSmall, autonomous, loosely coupled Reusable, communicating through well-defined interfaces Specialized to do one thing well Often implemented in an event-driven architecture When determining where to draw boundaries between microservices, consider:\nIntrinsic: technology used, performance, reliability, scalability Extrinsic: dependent functionality, rate of change, reusability Human: team ownership, managing cognitive load Technology Choices and Communication Scope Communication scope Technologies / patterns to consider Within a single microservice Amazon Simple Queue Service (Amazon SQS), AWS Step Functions Between microservices in a single service AWS CloudFormation cross-stack references, Amazon Simple Notification Service (Amazon SNS) Between services Amazon EventBridge, AWS Cloud Map, Amazon API Gateway The Pub/Sub Hub Using a hub-and-spoke architecture (or message broker) works well with a small number of tightly related microservices.\nEach microservice depends only on the hub Inter-microservice connections are limited to the contents of the published message Reduces the number of synchronous calls since pub/sub is a one-way asynchronous push Drawback: coordination and monitoring are needed to avoid microservices processing the wrong message.\nCore Microservice Provides foundational data and communication layer, including:\nAmazon S3 bucket for data Amazon DynamoDB for data catalog AWS Lambda to write messages into the data lake and catalog Amazon SNS topic as the hub Amazon S3 bucket for artifacts such as Lambda code Only allow indirect write access to the data lake through a Lambda function ‚Üí ensures consistency.\nFront Door Microservice Provides an API Gateway for external REST interaction Authentication \u0026amp; authorization based on OIDC via Amazon Cognito Self-managed deduplication mechanism using DynamoDB instead of SNS FIFO because: SNS deduplication TTL is only 5 minutes SNS FIFO requires SQS FIFO Ability to proactively notify the sender that the message is a duplicate Staging ER7 Microservice Lambda ‚Äútrigger‚Äù subscribed to the pub/sub hub, filtering messages by attribute Step Functions Express Workflow to convert ER7 ‚Üí JSON Two Lambdas: Fix ER7 formatting (newline, carriage return) Parsing logic Result or error is pushed back into the pub/sub hub New Features in the Solution 1. AWS CloudFormation Cross-Stack References Example outputs in the core microservice:\nOutputs: Bucket: Value: !Ref Bucket Export: Name: !Sub ${AWS::StackName}-Bucket ArtifactBucket: Value: !Ref ArtifactBucket Export: Name: !Sub ${AWS::StackName}-ArtifactBucket Topic: Value: !Ref Topic Export: Name: !Sub ${AWS::StackName}-Topic Catalog: Value: !Ref Catalog Export: Name: !Sub ${AWS::StackName}-Catalog CatalogArn: Value: !GetAtt Catalog.Arn Export: Name: !Sub ${AWS::StackName}-CatalogArn "},{"uri":"https://thanhtu18.github.io/workshop-template/5-workshop/5.8-cognito/5.8.4-app-client/","title":"Configure App Client","tags":[],"description":"","content":"Configure App Client When creating the User Pool, AWS Cognito automatically created a default App Client. In this step, we will configure this App Client to suit our application needs.\nNavigate to your User Pool in the Cognito console Go to App integration tab You will see the App Client has already been created Edit App Client Click on the App Client name to edit it Or click Edit if available Update app client information:\nApp client name: TaskManagementWebApp (if you want to change it) App client type: Public client Authentication flows: ‚úÖ ALLOW_USER_PASSWORD_AUTH ‚úÖ ALLOW_REFRESH_TOKEN_AUTH ‚úÖ ALLOW_USER_SRP_AUTH Configure Hosted UI Hosted UI settings:\nUse the Cognito Hosted UI: Enabled Domain type: Use a Cognito domain Cognito domain: taskmanagement-auth-[your-unique-id] Initial app client settings:\nAllowed callback URLs: http://localhost:3000/callback https://your-app-domain.com/callback Allowed sign-out URLs: http://localhost:3000/ https://your-app-domain.com/ OAuth 2.0 settings:\nAllowed OAuth flows: ‚úÖ Authorization code grant ‚úÖ Implicit grant Allowed OAuth scopes: ‚úÖ email ‚úÖ openid ‚úÖ profile Save Configuration Review all configurations Click Save changes Verify App Client Configuration After updating, note down the important information:\nApp Client Details:\nClient ID: [your-client-id] Hosted UI URL: https://taskmanagement-auth-[your-id].auth.us-east-1.amazoncognito.com The App Client is now configured and ready to handle authentication requests from your application. You can use the Client ID and Hosted UI URL to integrate with your web or mobile application.\n"},{"uri":"https://thanhtu18.github.io/workshop-template/5-workshop/","title":"Workshop","tags":[],"description":"","content":"Building a Task Management Platform with DevOps on AWS Serverless Overview AWS Serverless enables you to build and deploy applications without managing servers, automatically scales based on demand, and you only pay for what you use.\nIn this workshop, we will learn how to design, build, and deploy a complete task management platform TaskHub using serverless architecture and automated DevSecOps practices.\nWe will create a system that includes frontend, backend API, database, and a complete CI/CD pipeline. The workshop focuses on three main components to build a production-ready application on AWS:\nServerless Backend - Use AWS Lambda for business logic processing, API Gateway as the communication layer, DynamoDB for data storage, and Cognito for user authentication management with optimized costs.\nContent Delivery - Deploy Next.js application on S3, distribute globally via CloudFront with low latency, and protect with AWS WAF against common web attacks.\nDevOps Pipeline - Automate the build, test, and deploy process using CodePipeline and CodeBuild, integrate security scanning with CodeGuru, and manage infrastructure as code with CloudFormation.\nContent Workshop overview Prerequiste Deploying Serverless Functions with AWS Lambda Building an API Gateway with Amazon API Gateway Simple and Secure Object Storage with Amazon S3 Accelerating Content Delivery with Amazon CloudFront (CDN) Managing User Identity and Access with Amazon Cognito Managing Encryption Keys with AWS Key Management Service (KMS) SecretManager WAF "},{"uri":"https://thanhtu18.github.io/workshop-template/1-worklog/1.5-week5/","title":"Week 5 Worklog","tags":[],"description":"","content":"Week 5 Objectives: Master AWS Storage Services and Amazon S3 comprehensive features Learn AWS Backup and VM Import/Export strategies Practice Storage Gateway for hybrid cloud storage solutions Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 1 - Module 04-01: D·ªãch V·ª• L∆∞u Tr·ªØ Tr√™n AWS - Module 04-02: Amazon Simple Storage Service (S3) - Access Point - Storage Class - Module 04-03: S3 Static Website \u0026amp; CORS - Control Access - Object Key \u0026amp; Performance - Glacier - Module 04-04: Snow Family - Storage Gateway - Backup - Lab 13: AWS Backup Implementation + Create S3 Bucket for backup storage + Deploy backup infrastructure + Create comprehensive Backup Plan + Set up notification systems with SNS + Test restore operations + Clean up backup resources 06/10/2025 06/10/2025 https://000013.awsstudygroup.com/ 2 - Lab 14: VM Import/Export Complete Workflow + Set up VMware Workstation environment + Export Virtual Machine from on-premises + Upload virtual machine to AWS S3 + Import virtual machine to AWS EC2 + Deploy Instance from custom AMI + Configure S3 bucket ACL for VM storage + Export virtual machine from EC2 Instance + Resource cleanup on AWS Cloud 07/10/2025 07/10/2025 https://000014.awsstudygroup.com/ 3 - Practice Day: Hands-on Review + Practice AWS Storage Services configuration + Review S3 advanced features implementation + Test AWS Backup scenarios + Validate VM Import/Export workflow + Troubleshooting common issues 08/10/2025 08/10/2025 4 - Lab 24: Storage Gateway Implementation + Create Storage Gateway + Create File Shares configuration + Mount File shares on on-premises machine + Test hybrid storage functionality - Practice Session: + Storage Gateway troubleshooting + Performance optimization + Best practices review 09/10/2025 09/10/2025 https://000024.awsstudygroup.com/ 5 - Comprehensive Practice \u0026amp; Review + End-to-end AWS Storage services workflow + AWS Backup automation testing + VM migration scenario practice + Storage Gateway integration testing + Week 5 knowledge consolidation 10/10/2025 10/10/2025 üèÜ Week 5 Achievements Learned AWS Storage Services\nUnderstood AWS storage service types and categories Learned S3 Access Points and Storage Classes Configured S3 Static Website hosting and CORS Explored S3 performance optimization and Glacier Studied AWS Snow Family and Storage Gateway Completed AWS Backup Implementation\nCreated S3 bucket for backup storage Deployed backup infrastructure successfully Set up backup plans and SNS notifications Tested data restore operations Cleaned up backup resources properly Mastered VM Import/Export Process\nSet up VMware Workstation environment Exported and imported virtual machines Deployed instances from custom AMI Configured S3 bucket ACL for VM storage Completed VM migration with cleanup Implemented Storage Gateway\nCreated and configured Storage Gateway Set up File Shares successfully Mounted file shares on on-premises machines Tested hybrid storage functionality Gained Hands-on Experience\nPracticed AWS Storage services configuration Performed backup and restore testing Completed VM migration scenarios Learned troubleshooting techniques "},{"uri":"https://thanhtu18.github.io/workshop-template/3-blogstranslated/3.5-blog5/","title":"Blog 5","tags":[],"description":"","content":" ‚ö†Ô∏è Note: The information below is for reference purposes only. Please do not copy verbatim for your report, including this warning.\nGetting Started with Healthcare Data Lakes: Using Microservices Data lakes can help hospitals and healthcare facilities turn data into business insights, maintain business continuity, and protect patient privacy. A data lake is a centralized, managed, and secure repository to store all your data, both in its raw and processed forms for analysis. Data lakes allow you to break down data silos and combine different types of analytics to gain insights and make better business decisions.\nThis blog post is part of a larger series on getting started with setting up a healthcare data lake. In my final post of the series, ‚ÄúGetting Started with Healthcare Data Lakes: Diving into Amazon Cognito‚Äù, I focused on the specifics of using Amazon Cognito and Attribute Based Access Control (ABAC) to authenticate and authorize users in the healthcare data lake solution. In this blog, I detail how the solution evolved at a foundational level, including the design decisions I made and the additional features used. You can access the code samples for the solution in this Git repo for reference.\nArchitecture Guidance The main change since the last presentation of the overall architecture is the decomposition of a single service into a set of smaller services to improve maintainability and flexibility. Integrating a large volume of diverse healthcare data often requires specialized connectors for each format; by keeping them encapsulated separately as microservices, we can add, remove, and modify each connector without affecting the others. The microservices are loosely coupled via publish/subscribe messaging centered in what I call the ‚Äúpub/sub hub.‚Äù\nThis solution represents what I would consider another reasonable sprint iteration from my last post. The scope is still limited to the ingestion and basic parsing of HL7v2 messages formatted in Encoding Rules 7 (ER7) through a REST interface.\nThe solution architecture is now as follows:\nFigure 1. Overall architecture; colored boxes represent distinct services.\nWhile the term microservices has some inherent ambiguity, certain traits are common:\nSmall, autonomous, loosely coupled Reusable, communicating through well-defined interfaces Specialized to do one thing well Often implemented in an event-driven architecture When determining where to draw boundaries between microservices, consider:\nIntrinsic: technology used, performance, reliability, scalability Extrinsic: dependent functionality, rate of change, reusability Human: team ownership, managing cognitive load Technology Choices and Communication Scope Communication scope Technologies / patterns to consider Within a single microservice Amazon Simple Queue Service (Amazon SQS), AWS Step Functions Between microservices in a single service AWS CloudFormation cross-stack references, Amazon Simple Notification Service (Amazon SNS) Between services Amazon EventBridge, AWS Cloud Map, Amazon API Gateway The Pub/Sub Hub Using a hub-and-spoke architecture (or message broker) works well with a small number of tightly related microservices.\nEach microservice depends only on the hub Inter-microservice connections are limited to the contents of the published message Reduces the number of synchronous calls since pub/sub is a one-way asynchronous push Drawback: coordination and monitoring are needed to avoid microservices processing the wrong message.\nCore Microservice Provides foundational data and communication layer, including:\nAmazon S3 bucket for data Amazon DynamoDB for data catalog AWS Lambda to write messages into the data lake and catalog Amazon SNS topic as the hub Amazon S3 bucket for artifacts such as Lambda code Only allow indirect write access to the data lake through a Lambda function ‚Üí ensures consistency.\nFront Door Microservice Provides an API Gateway for external REST interaction Authentication \u0026amp; authorization based on OIDC via Amazon Cognito Self-managed deduplication mechanism using DynamoDB instead of SNS FIFO because: SNS deduplication TTL is only 5 minutes SNS FIFO requires SQS FIFO Ability to proactively notify the sender that the message is a duplicate Staging ER7 Microservice Lambda ‚Äútrigger‚Äù subscribed to the pub/sub hub, filtering messages by attribute Step Functions Express Workflow to convert ER7 ‚Üí JSON Two Lambdas: Fix ER7 formatting (newline, carriage return) Parsing logic Result or error is pushed back into the pub/sub hub New Features in the Solution 1. AWS CloudFormation Cross-Stack References Example outputs in the core microservice:\nOutputs: Bucket: Value: !Ref Bucket Export: Name: !Sub ${AWS::StackName}-Bucket ArtifactBucket: Value: !Ref ArtifactBucket Export: Name: !Sub ${AWS::StackName}-ArtifactBucket Topic: Value: !Ref Topic Export: Name: !Sub ${AWS::StackName}-Topic Catalog: Value: !Ref Catalog Export: Name: !Sub ${AWS::StackName}-Catalog CatalogArn: Value: !GetAtt Catalog.Arn Export: Name: !Sub ${AWS::StackName}-CatalogArn "},{"uri":"https://thanhtu18.github.io/workshop-template/5-workshop/5.6-s3/","title":"Set up S3","tags":[],"description":"","content":"WORKLOG: S3 ORIGIN CONFIGURATION (FULL) This Worklog outlines the steps for creating and configuring two (02) separate Amazon S3 Buckets, serving as the Origins for different system resources: Frontend Code and user-uploaded files.\n1. S3 Bucket 1 Configuration: taskhub-frontend-prod (Frontend Origin) This Bucket serves as the Origin for CloudFront, storing Frontend code (HTML, CSS, JS) and static assets.\n1.1. Access and Bucket Creation Log in to the AWS Console, find and select the Amazon S3 service. Click the \u0026ldquo;Create bucket\u0026rdquo; button. Fill in the general configuration information: Configuration Value Explanation Bucket name taskhub-frontend-prod The Bucket name must be globally unique. AWS Region Asia Pacific (Singapore) ap-southeast-1 Select the geographical region closest to the target users to optimize latency. 1.2. Object Ownership \u0026amp; Public Access Configuration Object Ownership: Select ACLs disabled (recommended). Purpose: Access permissions are managed centrally using a Bucket Policy, simplifying management. Block Public Access settings for this bucket: Action: Uncheck [ ] Block all public access (and all sub-options). Reason: This allows us to configure a Bucket Policy later to grant read-only access specifically to CloudFront OAC (Origin Access Control), ensuring S3 can function as a valid private Origin. 1.3. Versioning and Encryption Configuration Configuration Value Explanation Bucket Versioning Select: Disable Reduces Storage Costs as maintaining multiple versions of Frontend code is unnecessary. Default encryption Enable Ensures data is encrypted when stored (at rest). Encryption type Select: Server-side encryption with Amazon S3 managed keys (SSE-S3) The default, cost-effective encryption method. Bucket Key Select: Enable Minimizes Request Costs related to the encryption/decryption process. 2. S3 Bucket 2 Configuration: taskhub-files-prod (User Files Storage) This Bucket is used to store files uploaded by users (images, media\u0026hellip;). Maximum security is prioritized. The creation process is similar to the S3 Bucket above.\n2.1. Access and Bucket Creation Repeat the Bucket creation steps (Section 1.1). Bucket name: taskhub-files-prod AWS Region: Asia Pacific (Singapore) ap-southeast-1 2.2. Public Access Configuration (SECURITY DIFFERENCE) Block Public Access settings for this bucket: Keep [X] Block all public access (CHECK ALL). Reason: This is a Secure Bucket containing user data. Files MUST NOT be publicly accessible. Access will only be temporarily granted via Pre-signed URLs generated by the Backend API after authentication. 2.3. Versioning and Encryption Configuration (Similar) Configuration Value Explanation Bucket Versioning Select: Disable Prevents rapid storage cost increases when users update/delete files. Default encryption Enable Data encryption is mandatory for user data. Encryption type Select: Server-side encryption with Amazon S3 managed keys (SSE-S3) Standard S3 encryption. Bucket Key Select: Enable Reduces encryption request costs. "},{"uri":"https://thanhtu18.github.io/workshop-template/1-worklog/1.6-week6/","title":"Week 6 Worklog","tags":[],"description":"","content":"Week 6 Objectives: Understand Amazon FSx for Windows File Server architecture and use cases Deploy and configure multi-AZ FSx file systems (SSD \u0026amp; HDD) Practice performance testing, monitoring, and optimization on FSx Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 1 - Module 05-01: Amazon FSx for Windows File Server Overview + FSx architecture, integration with Active Directory + Use cases and basic operations 13/10/2025 13/10/2025 https://000025.awsstudygroup.com/ 2 - Lab 25 (Part 1): Amazon FSx for Windows File Server 2.1 Create Environment 2.2 Create an SSD Multi-AZ file system 2.3 Create an HDD Multi-AZ file system 14/10/2025 14/10/2025 https://000025.awsstudygroup.com/ 3 - Lab 25 (Part 2): File Shares \u0026amp; Performance 3. Create new file shares 4. Test Performance 5. Monitor Performance 15/10/2025 15/10/2025 https://000025.awsstudygroup.com/ 4 - Lab 25 (Part 3): Data Management Features 6. Enable data deduplication 7. Enable shadow copies 8. Manage user sessions and open files 9. Enable user storage quotas 11. Scale throughput capacity 12. Scale storage capacity 13. Delete environment 16/10/2025 16/10/2025 https://000025.awsstudygroup.com/ 5 - Practice \u0026amp; Review: + Review all Lab 25 steps (create, share, test, monitor) + Repeat main tasks on FSx to remember the flow + Note common issues and simple troubleshooting 17/10/2025 17/10/2025 https://000025.awsstudygroup.com/ üèÜ Week 6 Achievements Basic understanding of Amazon FSx for Windows File Server\nUnderstand FSx for storing files in Windows environments Know that FSx integrates with Active Directory and runs on AWS FSx Deployment\nCreated the lab environment Deployed SSD and HDD Multi-AZ file systems File Shares\nCreated file shares on FSx Connected from a Windows client and read/wrote files Did simple performance checks Data Features\nEnabled data deduplication and shadow copies Managed user sessions and open files Set simple user storage quotas "},{"uri":"https://thanhtu18.github.io/workshop-template/6-self-evaluation/","title":"Self-evaluation","tags":[],"description":"","content":"Throughout my internship at AMAZON WEB SERVICES VIETNAM COMPANY LIMITED from 09/09/2025 to 13/12/2025, I had the valuable opportunity to learn, develop, and apply theoretical knowledge acquired at university to a real professional work environment.\nDuring the internship, I actively participated in developing an intelligent AI chatbot project using advanced AWS services such as Amazon Bedrock, Lambda Functions, S3, and API Gateway, thereby not only enhancing my skills in proficient use of AWS cloud services, developing Python and JavaScript programming skills, improving effective teamwork capabilities, and learning to apply AI/ML to real-world solutions.\nRegarding work attitude, I always maintained a positive mindset, proactively completed assigned tasks with excellence, strictly adhered to company regulations, and was proactive in exchanging ideas and learning from colleagues as well as mentors to continuously improve work efficiency and output quality.\nTo reflect the most objective and honest assessment of my internship process, I would like to evaluate myself based on the following professional criteria:\nNo. Criteria Description Good Fair Average 1 Knowledge and Technical Skills Industry understanding, practical knowledge application, tool usage skills, work quality ‚òê ‚úÖ ‚òê 2 Learning Ability Absorbing new knowledge, learning quickly ‚òê ‚úÖ ‚òê 3 Initiative Self-research, taking on tasks without waiting for instructions ‚úÖ ‚òê ‚òê 4 Sense of Responsibility Completing work on time, ensuring quality ‚úÖ ‚òê ‚òê 5 Discipline Following schedules, regulations, work procedures ‚úÖ ‚òê ‚òê 6 Growth Mindset Willingness to receive feedback and improve oneself ‚òê ‚úÖ ‚òê 7 Communication Presenting ideas clearly, reporting work effectively ‚òê ‚úÖ ‚òê 8 Team Collaboration Working effectively with colleagues, participating in teams ‚úÖ ‚òê ‚òê 9 Professional Conduct Respecting colleagues, partners, work environment ‚úÖ ‚òê ‚òê 10 Problem-solving Thinking Identifying issues, proposing solutions, creativity ‚òê ‚úÖ ‚òê 11 Contribution to Projects/Organization Work effectiveness, improvement initiatives, team recognition ‚úÖ ‚òê ‚òê 12 Overall General assessment of the entire internship process ‚úÖ ‚òê ‚òê Areas for Improvement Enhance discipline, strictly comply with company regulations or any organization\u0026rsquo;s policies Improve problem-solving thinking approaches Learn better communication skills in daily interactions and work situations, handling various scenarios "},{"uri":"https://thanhtu18.github.io/workshop-template/3-blogstranslated/","title":"Translated Blogs","tags":[],"description":"","content":"This section will list and introduce the blogs you have translated. For example:\nBlog 1 - Develop and monitor a Spark application using existing data in Amazon S3 with Amazon SageMaker Unified Studio This blog explains how to develop and monitor Spark applications in the integrated environment of Amazon SageMaker Unified Studio. You will learn how to address challenges in managing big data analytics, such as fragmented development environments, complex resource management, and inconsistent monitoring. The article details how to set up Serverless EMR, develop Spark applications with the TPC-DS dataset in Jupyter notebooks, monitor performance using the Spark UI, and automate workflows with Amazon MWAA. This unified solution helps data teams focus on analytics instead of managing infrastructure.\nBlog 2 -Democratize data for timely decisions with text-to-SQL at Parcel Perform This blog explains how Parcel Perform uses generative AI and modern architecture to help operations teams query data quickly without relying on technical teams. It covers how to build a data lake to process billions of shipping event records per day, and how to use text-to-SQL and RAG workflows to automatically translate user questions into precise SQL queries.\nBlog 3 -Build a scalable AI video generator using Amazon SageMaker AI and CogVideoX This blog introduces how to build a scalable AI video generation system using Amazon SageMaker AI and the CogVideoX model. The solution transforms text and images into high-quality videos for a variety of purposes such as marketing, education, and product demonstrations, while leveraging AWS services for performance, security, and scalability.\nBlog 4 - \u0026hellip; This blog introduces how to start building a data lake in the healthcare sector by applying a microservices architecture. You will learn why data lakes are important for storing and analyzing diverse healthcare data (electronic medical records, lab test data, medical IoT devices‚Ä¶), how microservices help make the system more flexible, scalable, and easier to maintain. The article also guides you through the steps to set up the environment, organize the data processing pipeline, and ensure compliance with security \u0026amp; privacy standards such as HIPAA.\nBlog 5 - \u0026hellip; This blog introduces how to start building a data lake in the healthcare sector by applying a microservices architecture. You will learn why data lakes are important for storing and analyzing diverse healthcare data (electronic medical records, lab test data, medical IoT devices‚Ä¶), how microservices help make the system more flexible, scalable, and easier to maintain. The article also guides you through the steps to set up the environment, organize the data processing pipeline, and ensure compliance with security \u0026amp; privacy standards such as HIPAA.\nBlog 6 - \u0026hellip; This blog introduces how to start building a data lake in the healthcare sector by applying a microservices architecture. You will learn why data lakes are important for storing and analyzing diverse healthcare data (electronic medical records, lab test data, medical IoT devices‚Ä¶), how microservices help make the system more flexible, scalable, and easier to maintain. The article also guides you through the steps to set up the environment, organize the data processing pipeline, and ensure compliance with security \u0026amp; privacy standards such as HIPAA.\n"},{"uri":"https://thanhtu18.github.io/workshop-template/1-worklog/1.7-week7/","title":"Week 7 Worklog","tags":[],"description":"","content":"Week 7 Objectives: Understand Amazon S3 fundamentals and main use cases Create and configure S3 buckets for static website hosting Practice access control, CloudFront integration, and versioning Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 1 - Module: Starting with Amazon S3 + Learn about Amazon S3 basics and concepts (buckets, objects, regions) 20/10/2025 20/10/2025 https://000057.awsstudygroup.com/ 2 - Lab (Part 1): Create and prepare S3 bucket 2. Create S3 bucket 2.1 Download source code to the device (load data) 3. Enable static website feature 21/10/2025 21/10/2025 https://000057.awsstudygroup.com/ 3 - Lab (Part 2): Public access and testing 4. Configure public access block 5. Configure public objects 6. Test static website 22/10/2025 22/10/2025 https://000057.awsstudygroup.com/ 4 - Lab (Part 3): CloudFront, versioning and replication 7. Accelerate static websites with CloudFront 7.1 Block all public access 7.2 Configure Amazon CloudFront 7.3 Test Amazon CloudFront 8. Bucket versioning 9. Move objects 10. Replicate objects across multiple Regions 11. Clean up resources 12. Notes \u0026amp; Best Practices 23/10/2025 23/10/2025 https://000057.awsstudygroup.com/ 5 - Practice \u0026amp; Review: + Repeat full S3 static website workflow (create bucket, upload code, host website) + Practice configuring public access, CloudFront, and versioning again + Review notes and best practices from the lab 24/10/2025 24/10/2025 https://000057.awsstudygroup.com/ üèÜ Week 7 Achievements Amazon S3 Basics\nUnderstood what Amazon S3 is used for Learned key concepts: buckets, objects, regions and storage for static websites Created and Configured S3 Bucket\nCreated an S3 bucket for static website hosting Downloaded source code to the device and uploaded data to S3 Enabled the static website hosting feature on the bucket Managed Public Access and Testing\nConfigured S3 public access block settings Set objects to be publicly accessible where needed Opened and tested the static website URL Integrated with Amazon CloudFront\nBlocked direct public access to the S3 bucket Configured CloudFront distribution in front of the S3 website Tested CloudFront URL for faster and more secure access Used Versioning and Replication\nEnabled bucket versioning to track object changes Moved objects between buckets/folders Configured cross-Region replication for objects "},{"uri":"https://thanhtu18.github.io/workshop-template/5-workshop/5.7-cloudfront/","title":"Set up CloudFront","tags":[],"description":"","content":"COMPLETE CLOUDFRONT DISTRIBUTION CONFIGURATION This Worklog outlines the steps for configuring a CloudFront Distribution, focusing on setting up Origin Access Control (OAC) security and verifying the prerequisites on the S3 Bucket.\n1. S3 PREREQUISITES CHECK Before finalizing CloudFront setup, you must ensure the S3 Bucket taskhub-frontend-prod is protected and configured correctly.\n1.1. Static Website Hosting Status Check: The Properties tab of the S3 Bucket. Status: S3 static website hosting must be in the Disabled state. Explanation: Since CloudFront acts as the CDN, Static Website Hosting is not required on S3. S3 merely serves as the content repository (Origin). 1.2. Block Public Access \u0026amp; Bucket Policy Check: The Permissions tab of the S3 Bucket. Block public access (bucket settings): Must be in the On state (Block all public access). Bucket policy (Final Confirmation): Must contain the OAC Policy that is automatically updated by CloudFront. 2. CLOUDFRONT DISTRIBUTION CONFIGURATION 2.1. Step 1 \u0026amp; 2: Get started Configuration Value Explanation Action Click \u0026ldquo;Create distribution\u0026rdquo;. Start the CDN creation process. Plan Select Free Plan ($0/month). The free plan for the project. Distribution name taskhub-frontend-cdn A memorable name for the resource. Distribution type Single website or app. The appropriate type for a Frontend application. 2.2. Step 3: Specify origin (OAC Setup) 1. Specify Origin Origin type: Select Amazon S3. S3 origin: Select the S3 Bucket taskhub-frontend-prod. 2. Configure OAC (Automatic Security) 1.¬†Tick \u0026ldquo;Allow private S3 bucket access to CloudFront - Recommended\u0026rdquo;. 2.¬†Select Use recommended origin settings. * Explanation: Upon completing the Distribution creation, CloudFront will automatically update the S3 Bucket Policy to grant access via OAC.\n3. Cache Configuration Cache settings: Select Use recommended cache settings tailored to serving S3 content. Default Root Object: Enter index.html (Standard configuration for SPA). 2.3. Step 4 \u0026amp; 5: Security \u0026amp; TLS Configuration Applied Value Explanation WAF Uncheck paid features. Keeping defaults for the project. Viewer protocol policy Redirect HTTP to HTTPS Mandates the use of the secure protocol. Custom SSL certificate Default CloudFront Certificate Activates free HTTPS. 2.4. Step 6: Review and create 1. Review Origin (Source Confirmation) S3 origin: taskhub-frontend-prod. (Must ensure the correct Frontend Bucket is selected). Grant CloudFront access to origin: Must be Yes. (Confirms OAC functionality). 2. Final Configuration Price Class: Select Use all edge locations (best performance). Click the \u0026ldquo;Create distribution\u0026rdquo; button to deploy. 3. POST-DEPLOYMENT CHECK: OAC SECURITY CONFIRMATION After the Distribution status changes to Deploying, perform this crucial check to confirm that OAC is functional:\nAccess S3 Bucket taskhub-frontend-prod. Navigate to the \u0026ldquo;Permissions\u0026rdquo; tab. Find the \u0026ldquo;Bucket policy\u0026rdquo; section. Confirm: The Policy must be automatically updated and contain the JSON statement authorizing the CloudFront service. This confirms that OAC has blocked direct access and only allows CloudFront to read the files. "},{"uri":"https://thanhtu18.github.io/workshop-template/7-feedback/","title":"Share and Contribute Feedback","tags":[],"description":"","content":" Here, I would like to share my most sincere feelings about the internship experience at AWS Vietnam, hoping this feedback can help the FCJ team and future generations of interns have better experiences.\nGeneral Assessment 1. Working Environment The working environment at AWS Vietnam is truly impressive and professional. The modern office at Bitexco Financial Tower creates a sense of pride when working here. FCJ team members are always ready to assist when I encounter difficulties, even outside working hours. The workspace is tidy and comfortable, helping me focus better. The atmosphere is dynamic but not stressful, allowing me to maximize my abilities.\n2. Mentor / Admin Team Support The mentor provided very detailed guidance, explained clearly when I didn\u0026rsquo;t understand, and always encouraged me to ask questions. The admin team supported procedures, documentation, and created favorable conditions for my work. I highly appreciate that the mentor allowed me to try and solve problems by myself instead of just providing the answers.\n3. Suitability of Work to Academic Major The tasks I was assigned were suitable for the knowledge I learned at university, while also broadening my exposure to new areas I had not yet accessed. As a result, I both consolidated my foundational knowledge and learned practical skills.\n4. Learning Opportunities \u0026amp; Skill Development This is the point I appreciate the most. I not only learned technical skills but also developed professional skills, teamwork skills, and professional communication in a corporate environment. Mr. Hoang Van Kha also shared a lot of practical experience, helping me better orient my career path.\n5. Culture \u0026amp; Team Spirit The company culture is very positive: everyone respects each other, works seriously but with joy. When there\u0026rsquo;s an urgent project, everyone works together, providing support regardless of position. This made me feel like I was a part of the collective, even as just an intern.\n6. Intern Policies / Benefits The company provided internship allowance and offered flexibility in working hours when necessary. FCJ members were always ready to answer student questions, even outside of working hours. Furthermore, being able to participate in training workshops was a major plus.\nOther Questions What did you enjoy most during your internship? - The thing that made me most satisfied and fond of the experience was the supportive spirit and professional discipline of all the seniors in the FCJ team. Whenever I faced challenges, I received dedicated guidance, which helped me feel more confident and stable in my work.\nWhat do you think the company needs to improve for subsequent interns? - To help future interns feel more connected and close to each other, as well as to the company culture, I hope the company can enhance networking activities. Specifically, I propose: Organizing meetups for the members within our team so everyone has the opportunity to socialize, get acquainted, and work together more easily. Additionally, if there were more In-depth Training sessions on specific professional areas, it would be very beneficial for our career development orientation.\nIf recommending to friends, would you advise them to intern here? Why? - Absolutely YES! I would recommend it immediately. This is an ideal working environment: professional yet incredibly welcoming, with dedicated mentors, and clear work, suitable for developing comprehensive skills, from specialized knowledge to professional conduct.\nProposals \u0026amp; Wishes Do you have any proposals to improve the internship experience? - I propose focusing on community building and networking, specifically:\n* Increasing in-person meetups at the office: As I suggested, small team gatherings would help everyone feel closer, more bonded, and gain a clearer understanding of the company culture and the work itself.\n* Organizing additional experience sharing sessions or internal knowledge sharing among different departments to broaden the interns\u0026rsquo; knowledge and perspective.\nWould you like to continue this program in the future? - I am very eager to continue participating in this program or other company projects in the future, as I truly found a positive working environment and a great opportunity for self-development here.\nOther Feedback (Free sharing): - I would like to express my sincere thanks to the Company and the FCJ Team for creating such wonderful conditions and supporting me wholeheartedly. I hope the internship program will continue to improve, and that my feedback will bring even more value to the future generations of interns!\n"},{"uri":"https://thanhtu18.github.io/workshop-template/5-workshop/5.8-cognito/","title":"Setup AWS Cognito Authentication","tags":[],"description":"","content":"AWS Cognito User Authentication Workshop Overview AWS Cognito provides user identity and access management for web and mobile applications. It enables you to add user sign-up, sign-in, and access control to your applications quickly and easily.\nIn this workshop, you will learn how to:\nCreate and configure a Cognito User Pool Setup password policies and email verification Configure App Client with Hosted UI Implement basic email/password authentication Content Create Cognito User Pool Configure Password Policies Setup Email Verification Configure App Client "},{"uri":"https://thanhtu18.github.io/workshop-template/5-workshop/5.9-keymanagementservice/","title":"Set up KMS","tags":[],"description":"","content":"Objective Create a Customer Managed Key (CMK) on AWS KMS to:\nEncrypt data in DynamoDB Encrypt Secrets Manager Ensure DevSecOps standard ‚Äì data is encrypted using KMS Step 1 ‚Äì Access AWS Key Management Service (KMS) Log in to AWS Console Search for service: KMS Select Key Management Service Go to menu Customer managed keys Click Create a key Step 2 ‚Äì Configure Key At Key type, select Symmetric At Key usage, select Encrypt and decrypt Keep other options as default Click Next Step 3 ‚Äì Add Labels (Alias \u0026amp; Description) Alias Enter:\ntaskhub_kms Click Next\nStep 4 ‚Äì Define Key Administrative Permissions In the Key administrators list, select QuocBao Keep the option Allow key administrators to delete this key enabled Click Next The selected user at this step has full administrative permissions for the KMS Key:\nEdit key policy Enable / disable the key Delete the key Step 5 ‚Äì Define Key Usage Permissions In the Key users list, select QuocBao No need to add Other AWS accounts Click Next Step 6 ‚Äì Edit Key Policy At the Edit key policy step, click Edit Verify that the policy includes the following permission groups: Root account: kms:* User QuocBao is allowed to manage and use the key { \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Id\u0026#34;: \u0026#34;key-consolepolicy-3\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Sid\u0026#34;: \u0026#34;Enable IAM User Permissions\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Principal\u0026#34;: { \u0026#34;AWS\u0026#34;: \u0026#34;arn:aws:iam::166557634525:root\u0026#34; }, \u0026#34;Action\u0026#34;: \u0026#34;kms:*\u0026#34;, \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; }, { \u0026#34;Sid\u0026#34;: \u0026#34;Allow access for Key Administrators\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Principal\u0026#34;: { \u0026#34;AWS\u0026#34;: \u0026#34;arn:aws:iam::166557634525:user/QuocBao\u0026#34; }, \u0026#34;Action\u0026#34;: [ \u0026#34;kms:Create*\u0026#34;, \u0026#34;kms:Describe*\u0026#34;, \u0026#34;kms:Enable*\u0026#34;, \u0026#34;kms:List*\u0026#34;, \u0026#34;kms:Put*\u0026#34;, \u0026#34;kms:Update*\u0026#34;, \u0026#34;kms:Revoke*\u0026#34;, \u0026#34;kms:Disable*\u0026#34;, \u0026#34;kms:Get*\u0026#34;, \u0026#34;kms:Delete*\u0026#34;, \u0026#34;kms:TagResource\u0026#34;, \u0026#34;kms:UntagResource\u0026#34;, \u0026#34;kms:ScheduleKeyDeletion\u0026#34;, \u0026#34;kms:CancelKeyDeletion\u0026#34;, \u0026#34;kms:RotateKeyOnDemand\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; }, { \u0026#34;Sid\u0026#34;: \u0026#34;Allow use of the key\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Principal\u0026#34;: { \u0026#34;AWS\u0026#34;: \u0026#34;arn:aws:iam::166557634525:user/QuocBao\u0026#34; }, \u0026#34;Action\u0026#34;: [ \u0026#34;kms:Encrypt\u0026#34;, \u0026#34;kms:Decrypt\u0026#34;, \u0026#34;kms:ReEncrypt*\u0026#34;, \u0026#34;kms:GenerateDataKey*\u0026#34;, \u0026#34;kms:DescribeKey\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; }, { \u0026#34;Sid\u0026#34;: \u0026#34;Allow attachment of persistent resources\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Principal\u0026#34;: { \u0026#34;AWS\u0026#34;: \u0026#34;arn:aws:iam::166557634525:user/QuocBao\u0026#34; }, \u0026#34;Action\u0026#34;: [ \u0026#34;kms:CreateGrant\u0026#34;, \u0026#34;kms:ListGrants\u0026#34;, \u0026#34;kms:RevokeGrant\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34;, \u0026#34;Condition\u0026#34;: { \u0026#34;Bool\u0026#34;: { \u0026#34;kms:GrantIsForAWSResource\u0026#34;: \u0026#34;true\u0026#34; } } } ] } Modify the policy if necessary to match your actual account ARN Click Next AWS will automatically attach a valid policy to the key.\nStep 7 ‚Äì Review \u0026amp; Finish Review all configurations: Item Value Key type Symmetric Key usage Encrypt and decrypt Alias taskhub_kms Key Admin QuocBao Key User QuocBao Click Finish AWS will start creating the KMS Key.\nStep 8 ‚Äì Verify KMS Key Created Successfully After creation, go back to KMS ‚Üí Customer managed keys Verify the following information: Alias: taskhub_kms Status: Enabled Key type: Symmetric Key spec: SYMMETRIC_DEFAULT Key usage: Encrypt and decrypt The KMS Key has now been successfully created.\n"},{"uri":"https://thanhtu18.github.io/workshop-template/1-worklog/1.9-week9/","title":"Week 9 Worklog","tags":[],"description":"","content":"Week 9 Objectives: Learn how to use tags to manage AWS resources Practice creating Resource Groups and filtering resources by tags Manage access to EC2 using resource tags and IAM policies Understand IAM permission boundaries and how to limit user permissions Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 1 - Lab 27: Manage Resources Using Tags and Resource Groups 2. Using Tags 2.1 Use tags on Console - 2.1.1 Create EC2 instance with tag - 2.1.2 Manage tags on AWS resources - 2.1.3 Filter resources by tag 2.2 Use tags with CLI 3. Create a Resource Group 4. Clean up resources 03/11/2025 03/11/2025 https://000027.awsstudygroup.com/ 2 - Lab 28: Manage access to EC2 services with resource tags through IAM services 1. Introduction 2. Preparation - 2.1 Create IAM user 3. Create IAM Policy 4. Create IAM Role 5. Check Policy: - 5.1 Switch Roles - 5.2 Check IAM Policy - 5.2.1 Access EC2 console in Tokyo Region - 5.2.2 Access EC2 console in North Virginia Region - 5.2.3 Create EC2 instance with/without required tags - 5.2.4 Edit resource tag on EC2 instance - 5.2.5 Policy check results 6. Clean up resources 04/11/2025 04/11/2025 https://000028.awsstudygroup.com/ 3 - Practice: Review Lab 27 \u0026amp; Lab 28 + Repeat using tags on Console and CLI + Practice filtering resources and using Resource Groups + Practice tag-based access control for EC2 with IAM policies + Note simple best practices for tags and access control 05/11/2025 05/11/2025 https://000027.awsstudygroup.com/, https://000028.awsstudygroup.com/ 4 - Lab 30: Limitation of user rights with IAM Permission Boundary 1. Introduction 2. Preparation 3. Create restriction policy (permission boundary) 4. Create IAM limited user 5. Test IAM user limits 6. Clean up resources 06/11/2025 06/11/2025 https://000030.awsstudygroup.com/ 5 - Practice: Review Tag, IAM Policy, and Permission Boundary + Review how tags and Resource Groups help manage resources + Review tag-based access control to EC2 from Lab 28 + Review IAM permission boundary from Lab 30 + Summarize what was learned in Week 9 07/11/2025 07/11/2025 https://000027.awsstudygroup.com/, https://000028.awsstudygroup.com/, https://000030.awsstudygroup.com/ üèÜ Week 9 Achievements Tags and Resource Management\nUsed tags on the AWS Console to mark EC2 and other resources Filtered resources by tags to find them more easily Used CLI to work with tags and created Resource Groups Tag-based Access Control for EC2\nCreated IAM user, IAM policy, and IAM role for tag-based access Tested EC2 console access in different Regions (Tokyo, North Virginia) Tried creating EC2 instances with and without the required tags Edited resource tags and saw how policies allowed or denied actions IAM Permission Boundaries\nCreated a restriction policy as a permission boundary Created an IAM user with limited rights Tested what the user can and cannot do in the AWS console Practice and Cleanup\nPracticed using tags, Resource Groups, and IAM policies multiple times Cleaned up all lab resources after finishing Gained a clearer understanding of how tags and IAM work together to control access "},{"uri":"https://thanhtu18.github.io/workshop-template/1-worklog/1.8-week8/","title":"Week 8 Worklog","tags":[],"description":"","content":"Week 8 Objectives: Review AWS shared responsibility model and core security/IAM services Learn and practice AWS Security Hub with security standards Learn how to optimize EC2 costs using AWS Lambda automation Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 1 - Module 05: AWS Security and IAM Services + Module 05-01: Shared Responsibility Model + Module 05-02: AWS Identity and Access Management (IAM) + Module 05-03: Amazon Cognito + Module 05-04: AWS Organizations + Module 05-05: AWS Identity Center + Module 05-06: AWS Key Management Service (KMS) + Module 05-07: AWS Security Hub + Module 05-08: Hands-on and additional research 27/10/2025 27/10/2025 https://000018.awsstudygroup.com/ 2 - Lab 18 (Part 1): Get started with AWS Security Hub + Review Security Standards and AWS Foundational Security Best Practices + 2. Enable Security Hub 28/10/2025 28/10/2025 https://000018.awsstudygroup.com/ 3 - Lab 18 (Part 2): Evaluate Security Hub findings + 3. Check scores for each set of criteria + Review findings and understand security recommendations + 4. Clean up resources 29/10/2025 29/10/2025 https://000018.awsstudygroup.com/ 4 - Lab 22: Optimizing EC2 Costs with Lambda + 1. Understand how Lambda helps improve cost efficiency + 2. Preparation: - 2.1 Create VPC - 2.2 Create Security Group - 2.3 Create EC2 instance - 2.4 Configure Slack incoming webhooks + 3. Create tags for EC2 instance + 4. Create IAM Role for Lambda + 5. Create Lambda functions: - 5.1 Function to stop instances - 5.2 Function to start instances + 6. Check result and verify automation + 7. Clean up resources 30/10/2025 30/10/2025 https://000022.awsstudygroup.com/ 5 - Practice \u0026amp; Review: + Review all Module 05 security and IAM concepts + Practice again enabling Security Hub and reading basic findings + Review Lambda functions for EC2 cost optimization (start/stop instances) + Note simple best practices for security and cost management 31/10/2025 31/10/2025 https://000018.awsstudygroup.com/, https://000022.awsstudygroup.com/ üèÜ Week 8 Achievements AWS Security and IAM Basics\nReviewed the AWS shared responsibility model Learned basic concepts of IAM, Cognito, AWS Organizations, Identity Center, and KMS Used AWS Security Hub\nEnabled AWS Security Hub in the account Reviewed security standards and AWS Foundational Security Best Practices Saw security scores and basic findings for different checks Cleaned up the Security Hub lab resources Optimized EC2 Costs with Lambda\nCreated VPC, security group, and EC2 instance for the lab Configured Slack incoming webhooks for notifications Created tags to mark instances for automation Created IAM Role and Lambda functions to stop and start instances Tested the Lambda functions and verified that EC2 instances were controlled automatically Cleaned up EC2 and Lambda resources after the lab "},{"uri":"https://thanhtu18.github.io/workshop-template/1-worklog/1.10-week10/","title":"Week 10 Worklog","tags":[],"description":"","content":"Week 10 Objectives: Practice encryption at rest using AWS KMS with S3 and CloudTrail/Athena Review IAM roles, conditions, and access control patterns Practice IAM roles for applications (EC2 -\u0026gt; S3) Learn core AWS database services: Amazon RDS, Aurora, Redshift, ElastiCache Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 1 - Lab 33: Encrypt at rest with AWS KMS 1. Introduction 2. Preparation steps: - 2.1 Create policy and role - 2.2 Create group and user 3. Create AWS Key Management Service (KMS) key 4. Create Amazon S3: - 4.1 Create bucket - 4.2 Upload data to S3 5. Create AWS CloudTrail and Amazon Athena: - 5.1 Create CloudTrail - 5.2 Logging to CloudTrail - 5.3 Create Amazon Athena - 5.4 Retrieve data with Athena 6. Test and share encrypted data on S3 7. Resource cleanup 10/11/2025 10/11/2025 https://000033.awsstudygroup.com/ 2 - Lab 44: IAM Role \u0026amp; Condition 1. Introduction about IAM - 1.1 Request to AWS service - 1.2 Authenticate requests - 1.3 Assume Role process 2. Create IAM Group 3. Create IAM User: - 3.1 Create IAM users - 3.2 Check permissions 4. Configure Role Condition: - 4.1 Create Admin IAM Role - 4.2 Configure switch role - 4.3 Restrict role access: ‚Ä¢ 4.3.1 Limit switch role by IP ‚Ä¢ 4.3.2 Limit switch role by time 5. Clean up resources 11/11/2025 11/11/2025 https://000044.awsstudygroup.com/ 3 - Practice: Review Lab 33 \u0026amp; Lab 44 + Practice creating and using KMS keys to encrypt S3 data + Review CloudTrail and Athena queries for KMS/S3 activity + Practice IAM roles with conditions (IP, time) and switching roles + Write short notes about KMS, IAM Role, and conditions 12/11/2025 12/11/2025 https://000033.awsstudygroup.com/, https://000044.awsstudygroup.com/ 4 - Lab 48: Granting authorization for an application to access AWS services with an IAM role 1. Preparation: - 1.1 Create EC2 instance - 1.2 Create S3 bucket 2. Use access key: - 2.1 Generate IAM user and access key - 2.2 Use access key to access S3 from application 3. IAM role on EC2: - 3.1 Create IAM role - 3.2 Use IAM role from EC2 instead of access key 4. Clean up resources 13/11/2025 13/11/2025 https://000048.awsstudygroup.com/ 5 - Module 06: AWS Database Services + Module 06-01: Database Concepts Review + Module 06-02: Amazon RDS \u0026amp; Amazon Aurora + Module 06-03: Amazon Redshift \u0026amp; ElastiCache + Note key differences between relational, data warehouse, and in-memory cache services 14/11/2025 14/11/2025 üèÜ Week 10 Achievements KMS and Encryption at Rest\nUsed AWS KMS to create and manage encryption keys Encrypted data in S3 and verified access using CloudTrail and Athena Tested sharing encrypted S3 data and cleaned up lab resources IAM Roles and Conditions\nReviewed IAM basics: requests, authentication, assume role process Created IAM users, groups, and admin roles Applied role conditions to restrict access by IP address and time Application Access with IAM Roles\nCreated EC2 instance and S3 bucket for the lab Used IAM user access keys to access S3, then replaced them with IAM roles Verified that EC2 could access S3 securely using roles instead of long-term keys Database Service Overview\nReviewed core database concepts on AWS Learned basics of Amazon RDS and Aurora for relational databases Saw how Redshift and ElastiCache support analytics and caching workloads "},{"uri":"https://thanhtu18.github.io/workshop-template/1-worklog/1.11-week11/","title":"Week 11 Worklog","tags":[],"description":"","content":"Week 11 Objectives: Practice building and managing relational databases using Amazon RDS Explore database migration tools and scenarios with Lab 43 Build a simple data lake on AWS with S3, Glue, Athena, and QuickSight Learn to create and work with Amazon DynamoDB, including backups and migration Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 1 - Lab 05: Amazon Relational Database Service (Amazon RDS) 1. Introduction 2. Prerequisite steps: - 2.1 Create a VPC - 2.2 Create EC2 security group - 2.3 Create RDS security group - 2.4 Create DB subnet group 3. Create EC2 instance 4. Create RDS database instance 5. Application deployment 6. Backup and restore 7. Clean up resources 17/11/2025 17/11/2025 https://000005.awsstudygroup.com/ 2 - Lab 43: (DB migration \u0026amp; tools) 01. EC2 Connect RDP Client 02. EC2 Connect Fleet Manager 03. SQLSrv Src Config 04. Oracle connect SrcDB 05. Oracle config SrcDB 06. Drop Constraint 07. MSSQL to Aurora MySQL target config 08. MSSQL to Aurora MySQL create project 09. MSSQL to Aurora MySQL schema conversion 10. Oracle to MySQL schema conversion (part 1) 11. Create migration task and endpoint 12. Inspect S3 13. Create serverless migration 14. Create event notifications 15. Check logs 16. Troubleshoot memory pressure scenario 17. Troubleshoot table error scenario 18/11/2025 18/11/2025 Lab 43 3 - Lab 35: Data Lake on AWS 1. Introduce Data Lake (Big Data concept) 2. Preparation steps 3. Data collection and storage: - 3.1 Create S3 bucket - 3.2 Create Kinesis Firehose delivery stream (or similar) - 3.3 Create sample data 4. Create data catalog: - 4.1 Create AWS Glue crawler - 4.2 Check data/catalog 5. Data transformation 6. Analysis and visualization: - 6.1 Analyze with Athena - 6.2 Visualize with QuickSight 7. Clean up resources 19/11/2025 19/11/2025 https://000035.awsstudygroup.com/ 4 - Practice: Review Lab 05 \u0026amp; Lab 35 + Practice again creating RDS instances and connecting from EC2 + Review backup and restore steps for RDS + Practice creating S3 buckets, Glue crawlers, and running Athena queries + Write short notes comparing RDS vs data lake (S3 + Glue + Athena) 20/11/2025 20/11/2025 https://000005.awsstudygroup.com/, https://000035.awsstudygroup.com/ 5 - Lab 39: Learn to build and work with Amazon DynamoDB 1. LHOL: Hands-on lab for Amazon DynamoDB 2. Exploring DynamoDB 3. Exploring the DynamoDB Console 4. Backups 5. Cleanup 6. LADV: Advanced design patterns for Amazon DynamoDB 7. LMR: Building and deploying global serverless applications with DynamoDB 8. LEDA: Building a serverless event-driven architecture with DynamoDB 2025/11/21 2025/11/21 https://000039.awsstudygroup.com/ üèÜ Week 11 Achievements Amazon RDS basics\nCreated an RDS database and connected from an EC2 instance Tried simple application deployment and basic backup/restore Database migration\nGot familiar with migration tools (SQL Server/Oracle ‚Üí Aurora MySQL) Checked logs, S3 data and handled some sample issues Data Lake on AWS\nCreated an S3 bucket, Glue crawler and data catalog Queried data with Athena and viewed basic reports in QuickSight Amazon DynamoDB\nCreated DynamoDB tables and loaded sample data Read and wrote data using the CLI and Console, checked backup options "},{"uri":"https://thanhtu18.github.io/workshop-template/1-worklog/1.12-week12/","title":"Week 12 Worklog","tags":[],"description":"","content":"Week 12 Objectives: Practice building and inspecting cost-related data in AWS Get familiar with different ways to interact with AWS (CloudShell, Console, SDK) Use AWS Glue DataBrew and Cloud9 to prepare and transform data Build an end-to-end analytics pipeline with Glue, EMR, Athena, Kinesis Data Analytics, QuickSight, Lambda, and Redshift Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 1 - Lab 40: (Cost and usage data) 2.1 Preparing the database 2.2 Building a database 3.1 Data in the table 3.2 Cost 3.3 Tagging and cost allocation 3.4 Usage 3.5 Additional result query 4 Clean up resources 24/11/2025 24/11/2025 https://000040.awsstudygroup.com/ 2 - Lab 60 \u0026amp; Lab 70: AWS tools and data preparation Lab 60: CloudShell, Console, SDK Lab 70: Cloud9, dataset, S3, and AWS Glue DataBrew (profiling, clean \u0026amp; transform) 25/11/2025 25/11/2025 https://000060.awsstudygroup.com/, https://000070.awsstudygroup.com/ 3 - Practice: Review Lab 40 \u0026amp; Lab 70 + Practice checking cost/usage data and queries from Lab 40 + Practice using Cloud9 and S3 to prepare data + Practice creating and running basic DataBrew jobs for profiling and cleaning data + Write short notes about how these tools help with cost and data preparation 26/11/2025 26/11/2025 https://000040.awsstudygroup.com/, https://000070.awsstudygroup.com/ 4 - Lab 72: End-to-end analytics pipeline Preparatory steps, ingest \u0026amp; store, catalog data Transform with Glue (interactive \u0026amp; GUI), DataBrew, EMR Analysis with Athena and Kinesis Data Analytics Visualize in QuickSight, serve with Lambda, warehouse on Redshift 27/11/2025 27/11/2025 https://000072.awsstudygroup.com/ 5 - Lab 73 + Practice: Dashboards and review Build dashboard, improve dashboard, create interactive dashboard Practice reviewing pipeline parts from Lab 72 (Athena, Kinesis Data Analytics, QuickSight, Lambda, Redshift) Note simple best practices for building clear dashboards and reports 28/11/2025 28/11/2025 https://000073.awsstudygroup.com/, https://000072.awsstudygroup.com/ üèÜ Week 12 Achievements Cost and usage data basics\nBuilt a simple database to store cost and usage data Viewed table data, costs, tags, and usage information Ran additional queries and cleaned up lab resources AWS tools and data preparation\nUsed CloudShell, Console, and SDK to interact with AWS Created a Cloud9 environment and worked with datasets in S3 Used AWS Glue DataBrew for data profiling, cleaning, and transformation End-to-end analytics pipeline\nIngested and stored data, created a data catalog with Glue Transformed data using Glue (interactive, GUI), DataBrew, and EMR Analyzed data with Athena and Kinesis Data Analytics "},{"uri":"https://thanhtu18.github.io/workshop-template/5-workshop/5.17-secretmanager/","title":"Configure AWS Secrets Manager","tags":[],"description":"","content":"Objective Use AWS Secrets Manager to store configuration/secrets for the TaskHub system with the following requirements:\nSecrets are stored in JSON format (Key/value pairs ‚Äì Plaintext) Data is encrypted using KMS CMK: taskhub_kms (Optional) Enable automatic secret rotation using AWS Lambda Step 1 ‚Äì Access AWS Secrets Manager In the AWS Console, type Secrets Manager in the search box. Select Secrets Manager from the Services list. Step 2 ‚Äì Create a New Secret (Key/value pairs ‚Äì JSON) On the Secrets Manager main page, click Store a new secret. In Secret type, select: Other type of secret.\nIn the Key/value pairs section:\nSwitch from the Key/value tab to the Plaintext tab. Paste the following JSON content (demo for DynamoDB + KMS): { \u0026#34;Service\u0026#34;: \u0026#34;Amazon DynamoDB\u0026#34;, \u0026#34;Table\u0026#34;: \u0026#34;TaskHub Tables\u0026#34;, \u0026#34;Encryption\u0026#34;: \u0026#34;SSE-KMS\u0026#34;, \u0026#34;KMSKeyAlias\u0026#34;: \u0026#34;taskhub_kms\u0026#34;, \u0026#34;Purpose\u0026#34;: \u0026#34;Store users, projects, tasks\u0026#34;, \u0026#34;DataProtection\u0026#34;: \u0026#34;Encrypted at rest\u0026#34; } In the Encryption key field, select the KMS key:\ntaskhub_kms Click Next.\nStep 3 ‚Äì Configure Secret Name and Basic Information At the Configure secret step:\nSecret name:\nExample:\nprod/taskhub/secretmanager\nDescription (optional):\nMetadata for TaskHub DynamoDB encryption demo\nTags (optional): Skip for the workshop.\nResource permissions (optional): Keep default (IAM-based access control).\nReplicate secret (optional): Do not enable for this workshop.\nClick Next.\nStep 4 ‚Äì Configure Automatic Rotation (Optional) In a production environment, secrets are usually rotated every 30 days.\nIn this workshop, a shorter interval is configured for demonstration purposes.\nAt Configure rotation ‚Äì optional, enable: Automatic rotation In Rotation schedule: Select Schedule expression builder Time unit: Hours Hours: 23 (Optional) Window duration: 4h Keep Rotate immediately when the secret is stored checked In Rotation function: Select the Lambda function: taskhub-backend Click Next. Step 5 ‚Äì Review \u0026amp; Store the Secret On the Review step, verify the following information:\nSecret type: Other type of secret Encryption key: taskhub_kms Secret name: prod/taskhub/metadata Automatic rotation: Enabled Lambda rotation function: taskhub-backend Scroll down to the Sample code section:\nAWS provides built-in sample getSecret() functions for: Java JavaScript Python C# Go The TaskHub backend will use the corresponding SDK to retrieve secrets from AWS Secrets Manager instead of hard-coding them in source code. Click Store to complete the process.\nResult A new secret has been successfully created in AWS Secrets Manager. The secret content is stored in JSON format. The secret is: Encrypted at rest using KMS (taskhub_kms) Automatically rotatable using AWS Lambda The TaskHub backend can retrieve secrets via: AWS SDK IAM Role / Policy "},{"uri":"https://thanhtu18.github.io/workshop-template/5-workshop/5.18-waf/","title":"Set up Web ACL","tags":[],"description":"","content":"Step 1 ‚Äì Access AWS WAF \u0026amp; Shield Sign in to the AWS Console Search for the service: WAF \u0026amp; Shield Click AWS WAF\nFrom the left menu, select:\nProtection packs (web ACLs)\nClick the button:\nCreate protection pack (web ACL)\nStep 2 ‚Äì Select Application Type (Tell us about your app) 2.1 Select App Category Choose:\nAPI \u0026amp; integration services\nStep 3 ‚Äì Select the CloudFront Distribution to Protect Expand Select resources to protect Click Add resources Select:\nGlobal ‚Üí Add CloudFront or Amplify resources Check the CloudFront distribution of TaskHub (S3 frontend) Click Add Step 4 ‚Äì Select the Rule Pack Type Select: ‚úÖ Build your own pack from all of the protections AWS WAF offers\nIn the right panel, select: ‚úÖ AWS-managed rule group\nClick Next Step 5 ‚Äì Add Amazon IP Reputation List Select the rule: In Rule overrides, configure as follows: Rule Action AWSManagedIPReputationList ‚úÖ Block AWSManagedReconnaissanceList ‚úÖ Block AWSManagedIPDDoSList ‚úÖ Count Click Add rule ‚úÖ After this step, the rule will appear in the Add rules list.\nStep 6 ‚Äì Verify the Added Rule Verify that the following information appears:\nRule: AWSManagedRulesAmazonIpReputationList Status: Saved WCU: 25 WCU Step 7 ‚Äì Set the Web ACL Name In the Name and describe section:\nName: taskhub-waf Description: (leave blank or enter any description) Step 8 ‚Äì Create the Protection Pack (Web ACL) Click: Create protection pack (web ACL)\nWait for AWS to complete the Web ACL creation "},{"uri":"https://thanhtu18.github.io/workshop-template/3-blogstranslated/3.6-blog6/","title":"Blog 6","tags":[],"description":"","content":" ‚ö†Ô∏è Note: The information below is for reference purposes only. Please do not copy verbatim for your report, including this warning.\nGetting Started with Healthcare Data Lakes: Using Microservices Data lakes can help hospitals and healthcare facilities turn data into business insights, maintain business continuity, and protect patient privacy. A data lake is a centralized, managed, and secure repository to store all your data, both in its raw and processed forms for analysis. Data lakes allow you to break down data silos and combine different types of analytics to gain insights and make better business decisions.\nThis blog post is part of a larger series on getting started with setting up a healthcare data lake. In my final post of the series, ‚ÄúGetting Started with Healthcare Data Lakes: Diving into Amazon Cognito‚Äù, I focused on the specifics of using Amazon Cognito and Attribute Based Access Control (ABAC) to authenticate and authorize users in the healthcare data lake solution. In this blog, I detail how the solution evolved at a foundational level, including the design decisions I made and the additional features used. You can access the code samples for the solution in this Git repo for reference.\nArchitecture Guidance The main change since the last presentation of the overall architecture is the decomposition of a single service into a set of smaller services to improve maintainability and flexibility. Integrating a large volume of diverse healthcare data often requires specialized connectors for each format; by keeping them encapsulated separately as microservices, we can add, remove, and modify each connector without affecting the others. The microservices are loosely coupled via publish/subscribe messaging centered in what I call the ‚Äúpub/sub hub.‚Äù\nThis solution represents what I would consider another reasonable sprint iteration from my last post. The scope is still limited to the ingestion and basic parsing of HL7v2 messages formatted in Encoding Rules 7 (ER7) through a REST interface.\nThe solution architecture is now as follows:\nFigure 1. Overall architecture; colored boxes represent distinct services.\nWhile the term microservices has some inherent ambiguity, certain traits are common:\nSmall, autonomous, loosely coupled Reusable, communicating through well-defined interfaces Specialized to do one thing well Often implemented in an event-driven architecture When determining where to draw boundaries between microservices, consider:\nIntrinsic: technology used, performance, reliability, scalability Extrinsic: dependent functionality, rate of change, reusability Human: team ownership, managing cognitive load Technology Choices and Communication Scope Communication scope Technologies / patterns to consider Within a single microservice Amazon Simple Queue Service (Amazon SQS), AWS Step Functions Between microservices in a single service AWS CloudFormation cross-stack references, Amazon Simple Notification Service (Amazon SNS) Between services Amazon EventBridge, AWS Cloud Map, Amazon API Gateway The Pub/Sub Hub Using a hub-and-spoke architecture (or message broker) works well with a small number of tightly related microservices.\nEach microservice depends only on the hub Inter-microservice connections are limited to the contents of the published message Reduces the number of synchronous calls since pub/sub is a one-way asynchronous push Drawback: coordination and monitoring are needed to avoid microservices processing the wrong message.\nCore Microservice Provides foundational data and communication layer, including:\nAmazon S3 bucket for data Amazon DynamoDB for data catalog AWS Lambda to write messages into the data lake and catalog Amazon SNS topic as the hub Amazon S3 bucket for artifacts such as Lambda code Only allow indirect write access to the data lake through a Lambda function ‚Üí ensures consistency.\nFront Door Microservice Provides an API Gateway for external REST interaction Authentication \u0026amp; authorization based on OIDC via Amazon Cognito Self-managed deduplication mechanism using DynamoDB instead of SNS FIFO because: SNS deduplication TTL is only 5 minutes SNS FIFO requires SQS FIFO Ability to proactively notify the sender that the message is a duplicate Staging ER7 Microservice Lambda ‚Äútrigger‚Äù subscribed to the pub/sub hub, filtering messages by attribute Step Functions Express Workflow to convert ER7 ‚Üí JSON Two Lambdas: Fix ER7 formatting (newline, carriage return) Parsing logic Result or error is pushed back into the pub/sub hub New Features in the Solution 1. AWS CloudFormation Cross-Stack References Example outputs in the core microservice:\nOutputs: Bucket: Value: !Ref Bucket Export: Name: !Sub ${AWS::StackName}-Bucket ArtifactBucket: Value: !Ref ArtifactBucket Export: Name: !Sub ${AWS::StackName}-ArtifactBucket Topic: Value: !Ref Topic Export: Name: !Sub ${AWS::StackName}-Topic Catalog: Value: !Ref Catalog Export: Name: !Sub ${AWS::StackName}-Catalog CatalogArn: Value: !GetAtt Catalog.Arn Export: Name: !Sub ${AWS::StackName}-CatalogArn "},{"uri":"https://thanhtu18.github.io/workshop-template/categories/","title":"Categories","tags":[],"description":"","content":""},{"uri":"https://thanhtu18.github.io/workshop-template/tags/","title":"Tags","tags":[],"description":"","content":""}]